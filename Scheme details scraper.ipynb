{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6f7dd-c449-4e46-a172-7fc95ca51ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the links and names of the scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bc1b7-186b-43ba-be42-2b3814734588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    ElementClickInterceptedException\n",
    ")\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "CHECKPOINT_FILE = \"myscheme_checkpoint.xlsx\"\n",
    "OUTPUT_FILE = \"myscheme_data.xlsx\"\n",
    "TOTAL_PAGES = 329\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Configure and return a headless Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode (no UI)\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "         AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "         Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"\n",
    "    Loads existing data from CHECKPOINT_FILE if it exists,\n",
    "    and returns a DataFrame plus the last scraped page number.\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        df = pd.read_excel(CHECKPOINT_FILE)\n",
    "        if not df.empty:\n",
    "            # Find the maximum page we've already scraped\n",
    "            last_page = df['page'].max()\n",
    "            return df, last_page\n",
    "    # If file doesn't exist or is empty, start fresh\n",
    "    return pd.DataFrame(columns=[\"scheme_name\", \"scheme_link\", \"page\"]), 0\n",
    "\n",
    "def save_checkpoint(df):\n",
    "    \"\"\"\n",
    "    Saves the given DataFrame to the checkpoint file.\n",
    "    Overwrites any existing checkpoint.\n",
    "    \"\"\"\n",
    "    df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "\n",
    "def scrape_schemes_on_page(driver, current_page):\n",
    "    \"\"\"\n",
    "    Scrapes all scheme names and links on the current page.\n",
    "    Returns a list of dictionaries with keys: scheme_name, scheme_link, page.\n",
    "    \"\"\"\n",
    "    schemes_data = []\n",
    "\n",
    "    # Wait briefly for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    # We look for all <a> elements with href starting with \"/schemes/\"\n",
    "    # Each scheme has structure like:\n",
    "    # <a class=\"block text-lg ...\" href=\"/schemes/...\">\n",
    "    #     <span>Scheme Name</span>\n",
    "    # </a>\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[href^='/schemes/']\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(f\"Timeout: No schemes found on page {current_page} after waiting 10s.\")\n",
    "        return schemes_data\n",
    "\n",
    "    a_tags = driver.find_elements(By.CSS_SELECTOR, \"a[href^='/schemes/']\")\n",
    "    for a_tag in a_tags:\n",
    "        link = a_tag.get_attribute(\"href\")  # Full URL after expansion\n",
    "        # The scheme name is typically in a <span> inside the <a>\n",
    "        span_elem = a_tag.find_element(By.TAG_NAME, \"span\")\n",
    "        scheme_name = span_elem.text.strip() if span_elem else \"\"\n",
    "\n",
    "        if scheme_name:\n",
    "            schemes_data.append({\n",
    "                \"scheme_name\": scheme_name,\n",
    "                \"scheme_link\": link,\n",
    "                \"page\": current_page\n",
    "            })\n",
    "    return schemes_data\n",
    "\n",
    "def click_next_button(driver, current_page):\n",
    "    \"\"\"\n",
    "    Attempts to click the 'Next' pagination button or\n",
    "    the numeric page link for (current_page + 1).\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    # We'll attempt multiple methods:\n",
    "    # 1) Numeric link for page (current_page + 1)\n",
    "    # 2) Rightmost pagination arrow (the next button)\n",
    "\n",
    "    next_page_num = current_page + 1\n",
    "\n",
    "    # Approach 1: Numeric page link\n",
    "    page_link_xpath = f\"//li[text()='{next_page_num}'] | //a[text()='{next_page_num}']\"\n",
    "    links = driver.find_elements(By.XPATH, page_link_xpath)\n",
    "    if links:\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", links[0])\n",
    "            time.sleep(1)\n",
    "            links[0].click()\n",
    "            time.sleep(2)\n",
    "            return True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Approach 2: Next arrow button (SVG or last clickable in pagination)\n",
    "    # This might be an <svg> or a <div> with a cursor-pointer\n",
    "    try:\n",
    "        svg_next_buttons = driver.find_elements(\n",
    "            By.XPATH, \"//svg[contains(@class, 'cursor-pointer') or contains(@class, 'text-darkblue-900')]\"\n",
    "        )\n",
    "        # The site might have multiple SVGs; we assume the last one is \"Next\"\n",
    "        if svg_next_buttons:\n",
    "            button = svg_next_buttons[-1]\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", button)\n",
    "            time.sleep(1)\n",
    "            button.click()\n",
    "            time.sleep(2)\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking next arrow: {e}\")\n",
    "\n",
    "    return False\n",
    "\n",
    "def main_scraper():\n",
    "    \"\"\"\n",
    "    Main function to scrape all pages from 1 to TOTAL_PAGES,\n",
    "    resuming from the last scraped page if checkpoint file exists.\n",
    "    \"\"\"\n",
    "    # Load existing data from checkpoint\n",
    "    checkpoint_df, last_page_scraped = load_checkpoint()\n",
    "\n",
    "    # Start from the next page if we already scraped something\n",
    "    start_page = last_page_scraped + 1\n",
    "    if start_page > TOTAL_PAGES:\n",
    "        print(\"All pages already scraped. Exiting.\")\n",
    "        return\n",
    "\n",
    "    driver = setup_driver()\n",
    "    driver.get(\"https://www.myscheme.gov.in/search\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    current_page = 1\n",
    "    # If we need to jump to start_page (e.g. we left off at page 15),\n",
    "    # we repeatedly click next until we reach that page.\n",
    "    # (We can't do direct URL with ?page=... because the site doesn't load it that way.)\n",
    "    while current_page < start_page:\n",
    "        print(f\"Jumping to page {start_page}, currently at page {current_page}...\")\n",
    "        success = click_next_button(driver, current_page)\n",
    "        if success:\n",
    "            current_page += 1\n",
    "        else:\n",
    "            print(\"Failed to navigate pages while jumping. Exiting.\")\n",
    "            driver.quit()\n",
    "            return\n",
    "\n",
    "    # Now we can scrape from start_page to TOTAL_PAGES\n",
    "    while current_page <= TOTAL_PAGES:\n",
    "        print(f\"\\n=== Scraping Page {current_page} ===\")\n",
    "\n",
    "        # Scrape schemes\n",
    "        page_schemes = scrape_schemes_on_page(driver, current_page)\n",
    "        print(f\"Found {len(page_schemes)} schemes on page {current_page}\")\n",
    "\n",
    "        # Append new data to checkpoint DataFrame\n",
    "        new_data_df = pd.DataFrame(page_schemes)\n",
    "        checkpoint_df = pd.concat([checkpoint_df, new_data_df], ignore_index=True)\n",
    "\n",
    "        # Remove duplicates by scheme_link or scheme_name\n",
    "        checkpoint_df.drop_duplicates(subset=[\"scheme_link\", \"scheme_name\"], inplace=True)\n",
    "\n",
    "        # Save checkpoint after every page\n",
    "        save_checkpoint(checkpoint_df)\n",
    "        print(f\"Checkpoint saved. Total schemes so far: {len(checkpoint_df)}\")\n",
    "\n",
    "        # If we're not at the last page, click next\n",
    "        if current_page < TOTAL_PAGES:\n",
    "            success = click_next_button(driver, current_page)\n",
    "            if not success:\n",
    "                print(\"Could not click next. Ending scraping early.\")\n",
    "                break\n",
    "        current_page += 1\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Finally, save everything to the final output file\n",
    "    checkpoint_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nScraping complete! Total unique schemes: {len(checkpoint_df)}\")\n",
    "    print(f\"Data saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cffa0a-aa10-4ff0-a7fe-1bb40046addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the details of each scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf7697-c0b8-4cfd-876c-932af5cb23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import random\n",
    "\n",
    "# File paths\n",
    "INPUT_FILE = \"myscheme_data.xlsx\"  # Your existing file with scheme links\n",
    "OUTPUT_FILE = \"myscheme_details.xlsx\"  # New file for scheme details\n",
    "CHECKPOINT_FILE = \"details_checkpoint.xlsx\"  # Checkpoint file for tracking progress\n",
    "\n",
    "# Maximum number of retries for failed requests\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Configure and return a headless Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "         AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "         Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"\n",
    "    Load the checkpoint file to track progress.\n",
    "    Returns a DataFrame and the last processed index.\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        details_df = pd.read_excel(CHECKPOINT_FILE)\n",
    "        # Find the last processed scheme by looking at the index column\n",
    "        if not details_df.empty and 'original_index' in details_df.columns:\n",
    "            last_processed = details_df['original_index'].max()\n",
    "            return details_df, last_processed\n",
    "    # If file doesn't exist or is empty, start fresh\n",
    "    return pd.DataFrame(columns=[\"scheme_name\", \"scheme_link\", \"details_text\", \"original_index\"]), -1\n",
    "\n",
    "def save_checkpoint(df):\n",
    "    \"\"\"\n",
    "    Save current progress to the checkpoint file.\n",
    "    \"\"\"\n",
    "    df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "    print(f\"Checkpoint saved with {len(df)} schemes.\")\n",
    "\n",
    "def extract_details_section(driver, url):\n",
    "    \"\"\"\n",
    "    Extract the 'Details' section content from a scheme page.\n",
    "    Returns the extracted text or None if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Wait for page to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        \n",
    "        # First, find the \"Details\" heading which indicates the section\n",
    "        # This is based on the structure seen in the screenshots\n",
    "        # The details section might be inside a div with id=\"details\" or have a specific heading\n",
    "        \n",
    "        # Attempt to find the Details section content using various selectors\n",
    "        selectors = [\n",
    "            \"#details .markdown-options\", \n",
    "            \"h2:contains('Details') + div\", \n",
    "            \"div.markdown-options\", \n",
    "            \".markdown-options p\",\n",
    "            \"h2:contains('Details') ~ div\"\n",
    "        ]\n",
    "\n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                # Wait for the selector to be present\n",
    "                element = WebDriverWait(driver, 3).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                )\n",
    "                if element:\n",
    "                    # Extract text content\n",
    "                    details_text = element.text.strip()\n",
    "                    if details_text:\n",
    "                        return details_text\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        # If the CSS selectors fail, try XPath as a fallback\n",
    "        try:\n",
    "            # Looking for elements with text content after a heading with \"Details\"\n",
    "            details_element = driver.find_element(By.XPATH, \"//h2[contains(text(), 'Details')]/following-sibling::div[1]\")\n",
    "            if details_element:\n",
    "                return details_element.text.strip()\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        # If all attempts fail, try to get any text content in the page's main content area\n",
    "        main_content = driver.find_element(By.XPATH, \"//main\")\n",
    "        if main_content:\n",
    "            paragraphs = main_content.find_elements(By.TAG_NAME, \"p\")\n",
    "            if paragraphs:\n",
    "                return \"\\n\".join([p.text.strip() for p in paragraphs])\n",
    "                \n",
    "        return \"No details section found\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting details: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process scheme links and extract details.\n",
    "    \"\"\"\n",
    "    # Load the scheme links from the input file\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Input file {INPUT_FILE} not found.\")\n",
    "        return\n",
    "        \n",
    "    schemes_df = pd.read_excel(INPUT_FILE)\n",
    "    \n",
    "    # Check if the input file has the required columns\n",
    "    if 'scheme_name' not in schemes_df.columns or 'scheme_link' not in schemes_df.columns:\n",
    "        print(\"Input file missing required columns 'scheme_name' or 'scheme_link'.\")\n",
    "        return\n",
    "        \n",
    "    # Load checkpoint data\n",
    "    details_df, last_processed_index = load_checkpoint()\n",
    "    \n",
    "    # Set up the WebDriver\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    # Reset index and add original index column if not present\n",
    "    if 'original_index' not in schemes_df.columns:\n",
    "        schemes_df['original_index'] = schemes_df.index\n",
    "    \n",
    "    # Start from where we left off\n",
    "    start_index = last_processed_index + 1\n",
    "    \n",
    "    # Process schemes\n",
    "    total_schemes = len(schemes_df)\n",
    "    for idx, row in schemes_df.iloc[start_index:].iterrows():\n",
    "        scheme_name = row['scheme_name']\n",
    "        scheme_link = row['scheme_link']\n",
    "        original_index = row['original_index'] if 'original_index' in row else idx\n",
    "        \n",
    "        print(f\"\\n[{idx+1}/{total_schemes}] Processing: {scheme_name}\")\n",
    "        \n",
    "        # Try to extract details with retries\n",
    "        details_text = None\n",
    "        retries = 0\n",
    "        \n",
    "        while details_text is None and retries < MAX_RETRIES:\n",
    "            try:\n",
    "                details_text = extract_details_section(driver, scheme_link)\n",
    "                if not details_text:\n",
    "                    details_text = \"No details found\"\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {retries+1} failed: {e}\")\n",
    "                time.sleep(2 + random.random() * 3)  # Random delay between 2-5 seconds\n",
    "                retries += 1\n",
    "        \n",
    "        # If all retries failed\n",
    "        if details_text is None:\n",
    "            details_text = \"Failed to extract details\"\n",
    "        \n",
    "        # Add to details DataFrame\n",
    "        new_row = {\n",
    "            \"scheme_name\": scheme_name,\n",
    "            \"scheme_link\": scheme_link,\n",
    "            \"details_text\": details_text,\n",
    "            \"original_index\": original_index\n",
    "        }\n",
    "        \n",
    "        details_df = pd.concat([details_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        \n",
    "        # Save checkpoint every 10 schemes\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            save_checkpoint(details_df)\n",
    "            print(f\"Progress: {idx+1}/{total_schemes} schemes processed.\")\n",
    "            \n",
    "        # Add a random delay to avoid detection\n",
    "        time.sleep(1 + random.random() * 2)  # Random delay between 1-3 seconds\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "    \n",
    "    # Save final results\n",
    "    details_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    save_checkpoint(details_df)\n",
    "    \n",
    "    print(f\"\\nExtraction complete! Processed {len(details_df)} schemes.\")\n",
    "    print(f\"Results saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a01ac2-b835-4a7f-9902-25d88abf22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the missing details of each scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c8b62-67c0-4069-b161-ffb696b083d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import random\n",
    "\n",
    "# File paths\n",
    "INPUT_FILE = \"myscheme_details.xlsx\"  # Your existing file with scheme links and partial details\n",
    "OUTPUT_FILE = \"myscheme_details_complete.xlsx\"  # Updated file with complete details\n",
    "CHECKPOINT_FILE = \"details_retry_checkpoint.xlsx\"  # Checkpoint file for tracking progress\n",
    "\n",
    "# Maximum number of retries for failed requests\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Configure and return a headless Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "         AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "         Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def extract_details_section(driver, url):\n",
    "    \"\"\"\n",
    "    Extract the 'Details' section content from a scheme page.\n",
    "    Enhanced version with more precise selectors and wait times.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Wait for page to load\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        \n",
    "        # Add a fixed delay to ensure JavaScript rendering completes\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # First try to find the Details heading and then look for content after it\n",
    "        try:\n",
    "            # Look for the active tab or section that would contain \"Details\"\n",
    "            details_tab = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//h2[text()='Details'] | //button[text()='Details'] | //div[text()='Details'] | //a[text()='Details']\"))\n",
    "            )\n",
    "            # Click on the Details tab if it's not already active\n",
    "            if details_tab.tag_name.lower() in ['button', 'a'] and details_tab.is_displayed() and details_tab.is_enabled():\n",
    "                driver.execute_script(\"arguments[0].click();\", details_tab)\n",
    "                time.sleep(2)  # Wait for content to load after click\n",
    "        except:\n",
    "            # If we can't find or click a Details tab, continue with direct content extraction\n",
    "            pass\n",
    "        \n",
    "        # Multiple approaches to find the content, from most specific to most general\n",
    "        selectors = [\n",
    "            # Direct navigation to the details section\n",
    "            \"//div[@id='details']//p | //div[@id='details']//div[contains(@class, 'markdown-options')]\",\n",
    "            \n",
    "            # Based on your HTML structure in Image 3\n",
    "            \"//div[contains(@class, 'flex-1')]//div[contains(@class, 'markdown-options')]//div[contains(@class, 'mb-2')]\",\n",
    "            \n",
    "            # Find content after a details heading\n",
    "            \"//h2[text()='Details']/following-sibling::div[1]//p | //h2[text()='Details']/following-sibling::div[1]\",\n",
    "            \n",
    "            # Look for specific data-slate attributes as shown in Image 2\n",
    "            \"//span[@data-slate-string='true']\",\n",
    "            \n",
    "            # More generic approaches\n",
    "            \"//div[contains(@class, 'flex-1')]//p\",\n",
    "            \"//main//p\"\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                elements = driver.find_elements(By.XPATH, selector)\n",
    "                if elements:\n",
    "                    texts = [el.text.strip() for el in elements if el.text.strip()]\n",
    "                    if texts:\n",
    "                        return \"\\n\".join(texts)\n",
    "            except Exception as e:\n",
    "                print(f\"Selector {selector} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Last resort: capture any visible text in the main content area\n",
    "        try:\n",
    "            main_content = driver.find_element(By.TAG_NAME, \"main\")\n",
    "            paragraphs = main_content.find_elements(By.TAG_NAME, \"p\")\n",
    "            if paragraphs:\n",
    "                texts = [p.text.strip() for p in paragraphs if p.text.strip()]\n",
    "                if texts:\n",
    "                    return \"\\n\".join(texts)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        # If we still have nothing, try to get text directly using JavaScript\n",
    "        try:\n",
    "            script = \"\"\"\n",
    "            // Find Details section using various approaches\n",
    "            let detailsContent = '';\n",
    "            \n",
    "            // Approach 1: Find by ID\n",
    "            const detailsSection = document.getElementById('details');\n",
    "            if (detailsSection) {\n",
    "                detailsContent = detailsSection.innerText;\n",
    "            }\n",
    "            \n",
    "            // Approach 2: Find by heading\n",
    "            if (!detailsContent) {\n",
    "                const headings = Array.from(document.querySelectorAll('h2, h3'));\n",
    "                const detailsHeading = headings.find(h => h.innerText.includes('Details'));\n",
    "                if (detailsHeading) {\n",
    "                    let nextElem = detailsHeading.nextElementSibling;\n",
    "                    while (nextElem && !['H2', 'H3'].includes(nextElem.tagName)) {\n",
    "                        detailsContent += nextElem.innerText + '\\\\n';\n",
    "                        nextElem = nextElem.nextElementSibling;\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            // Approach 3: Find by tab content\n",
    "            if (!detailsContent) {\n",
    "                const tabContents = document.querySelectorAll('.tab-content, [role=\"tabpanel\"]');\n",
    "                for (const tab of tabContents) {\n",
    "                    if (tab.innerText.length > 20) {  // Assume content has reasonable length\n",
    "                        detailsContent = tab.innerText;\n",
    "                        break;\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return detailsContent.trim();\n",
    "            \"\"\"\n",
    "            js_result = driver.execute_script(script)\n",
    "            if js_result:\n",
    "                return js_result\n",
    "        except Exception as e:\n",
    "            print(f\"JavaScript extraction failed: {e}\")\n",
    "            \n",
    "        # If still nothing found\n",
    "        return \"No details found\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting details: {e}\")\n",
    "        return \"Error: \" + str(e)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process scheme links with \"No details found\" and extract details.\n",
    "    \"\"\"\n",
    "    # Load the scheme details from the input file\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Input file {INPUT_FILE} not found.\")\n",
    "        return\n",
    "        \n",
    "    schemes_df = pd.read_excel(INPUT_FILE)\n",
    "    \n",
    "    # Check if the input file has the required columns\n",
    "    required_columns = ['scheme_name', 'scheme_link', 'details_text', 'original_index']\n",
    "    if not all(col in schemes_df.columns for col in required_columns):\n",
    "        print(f\"Input file missing required columns. Required: {required_columns}\")\n",
    "        return\n",
    "    \n",
    "    # Filter only rows with \"No details found\"\n",
    "    no_details_df = schemes_df[schemes_df['details_text'].str.contains('No details found', na=False)]\n",
    "    has_details_df = schemes_df[~schemes_df['details_text'].str.contains('No details found', na=False)]\n",
    "    \n",
    "    print(f\"Found {len(no_details_df)} schemes with 'No details found'\")\n",
    "    print(f\"{len(has_details_df)} schemes already have details\")\n",
    "    \n",
    "    if len(no_details_df) == 0:\n",
    "        print(\"No schemes need processing. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the WebDriver\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    # Process schemes with no details\n",
    "    updated_rows = []\n",
    "    total_to_process = len(no_details_df)\n",
    "    \n",
    "    for idx, row in no_details_df.iterrows():\n",
    "        scheme_name = row['scheme_name']\n",
    "        scheme_link = row['scheme_link']\n",
    "        original_index = row['original_index']\n",
    "        \n",
    "        print(f\"\\n[{idx+1}/{total_to_process}] Processing: {scheme_name}\")\n",
    "        \n",
    "        # Try to extract details with retries\n",
    "        details_text = None\n",
    "        retries = 0\n",
    "        \n",
    "        while details_text is None and retries < MAX_RETRIES:\n",
    "            try:\n",
    "                details_text = extract_details_section(driver, scheme_link)\n",
    "                if not details_text or details_text == \"No details found\":\n",
    "                    print(f\"Attempt {retries+1}: Still no details found\")\n",
    "                    retries += 1\n",
    "                    time.sleep(2 + random.random() * 3)  # Random delay\n",
    "                    details_text = None  # Reset to trigger retry\n",
    "                else:\n",
    "                    print(f\"Successfully extracted details: {len(details_text)} characters\")\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {retries+1} failed: {e}\")\n",
    "                time.sleep(2 + random.random() * 3)\n",
    "                retries += 1\n",
    "        \n",
    "        # If all retries failed\n",
    "        if details_text is None:\n",
    "            details_text = \"No details found after multiple attempts\"\n",
    "        \n",
    "        # Update the row\n",
    "        updated_row = row.copy()\n",
    "        updated_row['details_text'] = details_text\n",
    "        updated_rows.append(updated_row)\n",
    "        \n",
    "        # Save checkpoint every 5 schemes\n",
    "        if (idx + 1) % 5 == 0 or (idx + 1) == total_to_process:\n",
    "            # Create temporary DataFrame with processed rows\n",
    "            processed_df = pd.DataFrame(updated_rows)\n",
    "            # Merge with schemes that already have details\n",
    "            temp_df = pd.concat([has_details_df, processed_df])\n",
    "            temp_df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "            print(f\"Checkpoint saved. Processed {idx+1}/{total_to_process} no-details schemes.\")\n",
    "            \n",
    "        # Add a random delay to avoid detection\n",
    "        time.sleep(1 + random.random() * 2)\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "    \n",
    "    # Combine updated rows with rows that already had details\n",
    "    final_df = pd.concat([has_details_df, pd.DataFrame(updated_rows)])\n",
    "    \n",
    "    # Sort by original index to maintain the original order\n",
    "    final_df = final_df.sort_values('original_index')\n",
    "    \n",
    "    # Save final results\n",
    "    final_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nExtraction complete! Updated {len(updated_rows)} schemes.\")\n",
    "    print(f\"Results saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d00e3-a3a2-4e7f-af12-25c07ee76413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the benefit section of each scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62037bf-41f0-4a16-9271-5ce5427f8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import random\n",
    "\n",
    "# File paths\n",
    "INPUT_FILE = \"myscheme_details_complete.xlsx\"  # Your existing file with scheme links and details\n",
    "OUTPUT_FILE = \"myscheme_details_with_benefits.xlsx\"  # Updated file with benefits added\n",
    "CHECKPOINT_FILE = \"benefits_retry_checkpoint.xlsx\"  # Checkpoint file for tracking progress\n",
    "\n",
    "# Maximum number of retries for failed requests\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Configure and return a headless Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "         AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "         Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def check_for_table(driver):\n",
    "    \"\"\"\n",
    "    Check if there's a table in the Benefits section.\n",
    "    Returns True if a table is found, False otherwise.\n",
    "    \"\"\"\n",
    "    # First check for tables in the active tab or benefits section\n",
    "    specific_selectors = [\n",
    "        \"//div[contains(@class, 'active')]//table\",\n",
    "        \"//div[@id='benefits']//table\",\n",
    "        \"//div[contains(@class, 'benefits')]//table\",\n",
    "        \"//h2[contains(text(), 'Benefits')]/following-sibling::div//table\",\n",
    "        \"//section[contains(@class, 'benefits')]//table\"\n",
    "    ]\n",
    "    \n",
    "    for selector in specific_selectors:\n",
    "        try:\n",
    "            tables = driver.find_elements(By.XPATH, selector)\n",
    "            if tables and len(tables) > 0 and tables[0].is_displayed():\n",
    "                print(f\"Table detected in benefits section using selector: {selector}\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # JavaScript detection - more comprehensive\n",
    "    script = \"\"\"\n",
    "    function detectBenefitsTable() {\n",
    "        // Find benefits section first\n",
    "        const benefitsSection = document.getElementById('benefits') || \n",
    "                               document.querySelector('.benefits') ||\n",
    "                               document.querySelector('[data-section=\"benefits\"]');\n",
    "        \n",
    "        // Check if benefits section contains a table\n",
    "        if (benefitsSection && benefitsSection.querySelector('table')) {\n",
    "            return true;\n",
    "        }\n",
    "        \n",
    "        // Check if any heading contains \"Benefits\" and next siblings contain table\n",
    "        const headings = Array.from(document.querySelectorAll('h1, h2, h3, h4, h5'));\n",
    "        const benefitsHeading = headings.find(h => h.innerText.toLowerCase().includes('benefits'));\n",
    "        \n",
    "        if (benefitsHeading) {\n",
    "            // Look through next siblings until we hit another heading\n",
    "            let currentNode = benefitsHeading.nextElementSibling;\n",
    "            while (currentNode && !['H1', 'H2', 'H3', 'H4', 'H5'].includes(currentNode.tagName)) {\n",
    "                if (currentNode.tagName === 'TABLE' || currentNode.querySelector('table')) {\n",
    "                    return true;\n",
    "                }\n",
    "                currentNode = currentNode.nextElementSibling;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Check active tabs for tables\n",
    "        const activeTabs = document.querySelectorAll('[role=\"tabpanel\"].active, .tab-content.active, .tab-pane.active');\n",
    "        for (const tab of activeTabs) {\n",
    "            if (tab.querySelector('table')) {\n",
    "                return true;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return false;\n",
    "    }\n",
    "    \n",
    "    return detectBenefitsTable();\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        has_table = driver.execute_script(script)\n",
    "        if has_table:\n",
    "            print(\"Table detected via JavaScript\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"JavaScript table detection failed: {e}\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "def extract_table_contents(driver):\n",
    "    \"\"\"\n",
    "    Extract the contents of tables in the Benefits section and format them nicely.\n",
    "    \"\"\"\n",
    "    table_data = []\n",
    "    \n",
    "    # Use various selectors to find tables in benefits sections\n",
    "    table_selectors = [\n",
    "        \"//div[contains(@class, 'active')]//table\",\n",
    "        \"//div[@id='benefits']//table\",\n",
    "        \"//div[contains(@class, 'benefits')]//table\",\n",
    "        \"//h2[contains(text(), 'Benefits')]/following-sibling::div//table\",\n",
    "        \"//section[contains(@class, 'benefits')]//table\"\n",
    "    ]\n",
    "    \n",
    "    for selector in table_selectors:\n",
    "        try:\n",
    "            tables = driver.find_elements(By.XPATH, selector)\n",
    "            for table in tables:\n",
    "                if table.is_displayed():\n",
    "                    # Try to extract rows\n",
    "                    rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    if rows:\n",
    "                        for row in rows:\n",
    "                            # Extract header cells and data cells\n",
    "                            headers = row.find_elements(By.TAG_NAME, \"th\")\n",
    "                            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            \n",
    "                            if headers and len(headers) > 0:\n",
    "                                # It's a header row\n",
    "                                header_text = \" | \".join([h.text.strip() for h in headers if h.text.strip()])\n",
    "                                if header_text:\n",
    "                                    table_data.append(f\"Header: {header_text}\")\n",
    "                            elif cells and len(cells) > 0:\n",
    "                                # It's a data row\n",
    "                                cell_text = \" | \".join([c.text.strip() for c in cells if c.text.strip()])\n",
    "                                if cell_text:\n",
    "                                    table_data.append(f\"• {cell_text}\")\n",
    "                    \n",
    "                    # If we found data, return it\n",
    "                    if table_data:\n",
    "                        return \"\\n\".join(table_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting table with selector {selector}: {e}\")\n",
    "    \n",
    "    # Try using JavaScript for more complex tables\n",
    "    script = \"\"\"\n",
    "    function extractTableData() {\n",
    "        // Find tables in benefits sections\n",
    "        const tables = [];\n",
    "        \n",
    "        // Find benefits section first\n",
    "        const benefitsSections = [\n",
    "            document.getElementById('benefits'),\n",
    "            ...Array.from(document.querySelectorAll('.benefits, [data-section=\"benefits\"]'))\n",
    "        ].filter(Boolean);\n",
    "        \n",
    "        // Add tables from benefits sections\n",
    "        benefitsSections.forEach(section => {\n",
    "            section.querySelectorAll('table').forEach(table => tables.push(table));\n",
    "        });\n",
    "        \n",
    "        // Add tables from active tabs\n",
    "        document.querySelectorAll('[role=\"tabpanel\"].active, .tab-content.active, .tab-pane.active')\n",
    "            .forEach(tab => tab.querySelectorAll('table').forEach(table => tables.push(table)));\n",
    "        \n",
    "        // Add tables after Benefits headings\n",
    "        const headings = Array.from(document.querySelectorAll('h1, h2, h3, h4, h5'))\n",
    "            .filter(h => h.innerText.toLowerCase().includes('benefits'));\n",
    "            \n",
    "        headings.forEach(heading => {\n",
    "            let element = heading.nextElementSibling;\n",
    "            while (element && !element.tagName.match(/^H[1-5]$/)) {\n",
    "                element.querySelectorAll('table').forEach(table => tables.push(table));\n",
    "                element = element.nextElementSibling;\n",
    "            }\n",
    "        });\n",
    "        \n",
    "        // If no tables found\n",
    "        if (tables.length === 0) return '';\n",
    "        \n",
    "        // Extract data from tables\n",
    "        let result = [];\n",
    "        \n",
    "        tables.forEach((table, tableIndex) => {\n",
    "            // Add table number if multiple tables\n",
    "            if (tables.length > 1) {\n",
    "                result.push(`Table ${tableIndex + 1}:`);\n",
    "            }\n",
    "            \n",
    "            // Get rows\n",
    "            const rows = table.querySelectorAll('tr');\n",
    "            rows.forEach((row, rowIndex) => {\n",
    "                const headerCells = row.querySelectorAll('th');\n",
    "                const dataCells = row.querySelectorAll('td');\n",
    "                \n",
    "                // Process header cells\n",
    "                if (headerCells.length > 0) {\n",
    "                    const headerText = Array.from(headerCells)\n",
    "                        .map(cell => cell.innerText.trim())\n",
    "                        .filter(text => text)\n",
    "                        .join(' | ');\n",
    "                    \n",
    "                    if (headerText) {\n",
    "                        result.push(`Header: ${headerText}`);\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                // Process data cells\n",
    "                if (dataCells.length > 0) {\n",
    "                    const cellText = Array.from(dataCells)\n",
    "                        .map(cell => cell.innerText.trim())\n",
    "                        .filter(text => text)\n",
    "                        .join(' | ');\n",
    "                    \n",
    "                    if (cellText) {\n",
    "                        result.push(`• ${cellText}`);\n",
    "                    }\n",
    "                }\n",
    "            });\n",
    "        });\n",
    "        \n",
    "        return result.join('\\\\n');\n",
    "    }\n",
    "    \n",
    "    return extractTableData();\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        js_result = driver.execute_script(script)\n",
    "        if js_result and len(js_result) > 10:\n",
    "            return js_result\n",
    "    except Exception as e:\n",
    "        print(f\"JavaScript table extraction failed: {e}\")\n",
    "    \n",
    "    return \"Table detected but content could not be extracted\"\n",
    "\n",
    "def extract_benefits_section(driver, url):\n",
    "    \"\"\"\n",
    "    Extract the 'Benefits' section content from a scheme page.\n",
    "    Handles both bullet-point style benefits and extracts tables.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Wait for page to load\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        \n",
    "        # Add a fixed delay to ensure JavaScript rendering completes\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # First approach: Look for tabs and click on \"Benefits\" if it exists\n",
    "        try:\n",
    "            # Use a more comprehensive XPath to find Benefits tab with case-insensitive matching\n",
    "            benefits_tab = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((\n",
    "                    By.XPATH, \n",
    "                    \"//a[contains(translate(text(),'benefits','BENEFITS'),'BENEFITS')] | \" +\n",
    "                    \"//button[contains(translate(text(),'benefits','BENEFITS'),'BENEFITS')] | \" +\n",
    "                    \"//div[contains(translate(text(),'benefits','BENEFITS'),'BENEFITS') and not(ancestor::*[contains(@class, 'content')])] | \" +\n",
    "                    \"//span[contains(translate(text(),'benefits','BENEFITS'),'BENEFITS') and not(ancestor::*[contains(@class, 'content')])]\"\n",
    "                ))\n",
    "            )\n",
    "            \n",
    "            # Check if element is clickable before clicking\n",
    "            if benefits_tab.is_displayed() and benefits_tab.is_enabled():\n",
    "                # Use JavaScript click for more reliability\n",
    "                driver.execute_script(\"arguments[0].click();\", benefits_tab)\n",
    "                print(\"Clicked on Benefits tab\")\n",
    "                time.sleep(3)  # Wait longer for content to load after click\n",
    "        except Exception as e:\n",
    "            print(f\"Could not find or click Benefits tab: {e}\")\n",
    "        \n",
    "        # Check for tables in benefits section first\n",
    "        if check_for_table(driver):\n",
    "            print(\"Table found in benefits section, extracting table content...\")\n",
    "            table_content = extract_table_contents(driver)\n",
    "            if table_content and len(table_content) > 10:\n",
    "                return table_content\n",
    "\n",
    "        # Check for section heading first, this is the most reliable indicator\n",
    "        try:\n",
    "            # Look for Benefits heading (h1, h2, h3, or h4)\n",
    "            headings = driver.find_elements(By.XPATH, \n",
    "                \"//h1[contains(translate(text(),'Benefits','BENEFITS'),'BENEFITS')] | \" +\n",
    "                \"//h2[contains(translate(text(),'Benefits','BENEFITS'),'BENEFITS')] | \" +\n",
    "                \"//h3[contains(translate(text(),'Benefits','BENEFITS'),'BENEFITS')] | \" +\n",
    "                \"//h4[contains(translate(text(),'Benefits','BENEFITS'),'BENEFITS')]\")\n",
    "            \n",
    "            if headings:\n",
    "                # Get the first heading that contains \"Benefits\"\n",
    "                benefits_heading = headings[0]\n",
    "                print(f\"Found Benefits heading: {benefits_heading.text}\")\n",
    "                \n",
    "                # Try to find the section content based on the heading\n",
    "                section_content = None\n",
    "                \n",
    "                # Method 1: Get sibling div or section content\n",
    "                try:\n",
    "                    # Look for the next siblings or content that follows the heading\n",
    "                    next_element = driver.execute_script(\"\"\"\n",
    "                        function getNextElements(element, maxElements = 5) {\n",
    "                            let result = '';\n",
    "                            let currentElement = element;\n",
    "                            let count = 0;\n",
    "                            \n",
    "                            while (currentElement.nextElementSibling && count < maxElements) {\n",
    "                                let nextElem = currentElement.nextElementSibling;\n",
    "                                \n",
    "                                // Stop if we hit another heading or a new section\n",
    "                                if (nextElem.tagName.startsWith('H') || \n",
    "                                    nextElem.className.includes('section') ||\n",
    "                                    (nextElem.innerText && (\n",
    "                                        nextElem.innerText.includes('Eligibility') || \n",
    "                                        nextElem.innerText.includes('Documents Required') || \n",
    "                                        nextElem.innerText.includes('Application Process')\n",
    "                                    ))) {\n",
    "                                    break;\n",
    "                                }\n",
    "                                \n",
    "                                // Add element content if it's visible and has text\n",
    "                                if (nextElem.offsetParent !== null && nextElem.innerText.trim()) {\n",
    "                                    result += nextElem.innerText + '\\\\n';\n",
    "                                }\n",
    "                                \n",
    "                                currentElement = nextElem;\n",
    "                                count++;\n",
    "                            }\n",
    "                            \n",
    "                            return result.trim();\n",
    "                        }\n",
    "                        return getNextElements(arguments[0]);\n",
    "                    \"\"\", benefits_heading)\n",
    "                    \n",
    "                    if next_element and len(next_element) > 10:\n",
    "                        section_content = next_element\n",
    "                except Exception as e:\n",
    "                    print(f\"Error getting next elements: {e}\")\n",
    "                \n",
    "                # Method 2: Find nearest container div\n",
    "                if not section_content:\n",
    "                    try:\n",
    "                        # Get parent div or section that might contain content\n",
    "                        parent_container = driver.execute_script(\"\"\"\n",
    "                            function findContentContainer(element) {\n",
    "                                // Get parent container that likely contains all content\n",
    "                                let current = element;\n",
    "                                \n",
    "                                // Look up for a container (div, section) that would contain content\n",
    "                                while (current && current.tagName !== 'BODY') {\n",
    "                                    current = current.parentElement;\n",
    "                                    \n",
    "                                    // Skip tiny containers\n",
    "                                    if (current && current.innerText.length < 20) continue;\n",
    "                                    \n",
    "                                    // If we find a suitable container with content\n",
    "                                    if (current && \n",
    "                                        (current.tagName === 'DIV' || current.tagName === 'SECTION') &&\n",
    "                                        current.innerText.length > 50) {\n",
    "                                        \n",
    "                                        // Extract only relevant part\n",
    "                                        const fullText = current.innerText;\n",
    "                                        const headingText = element.innerText;\n",
    "                                        const startPos = fullText.indexOf(headingText);\n",
    "                                        \n",
    "                                        if (startPos >= 0) {\n",
    "                                            // Find where the next section likely begins\n",
    "                                            const endPos = fullText.indexOf('Eligibility', startPos + headingText.length);\n",
    "                                            if (endPos > startPos) {\n",
    "                                                return fullText.substring(startPos + headingText.length, endPos).trim();\n",
    "                                            } else {\n",
    "                                                // Just get content after heading\n",
    "                                                return fullText.substring(startPos + headingText.length).trim();\n",
    "                                            }\n",
    "                                        }\n",
    "                                    }\n",
    "                                }\n",
    "                                return '';\n",
    "                            }\n",
    "                            return findContentContainer(arguments[0]);\n",
    "                        \"\"\", benefits_heading)\n",
    "                        \n",
    "                        if parent_container and len(parent_container) > 10:\n",
    "                            section_content = parent_container\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error getting parent container: {e}\")\n",
    "                \n",
    "                # If content was found, return it\n",
    "                if section_content:\n",
    "                    return section_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Benefits heading: {e}\")\n",
    "\n",
    "        # Comprehensive approach to find bullet points\n",
    "        selectors = [\n",
    "            # Active tab content with bullet points\n",
    "            \"//div[contains(@class, 'active') or @role='tabpanel']//ul/li\",\n",
    "            \n",
    "            # Benefits section with bullet points\n",
    "            \"//div[@id='benefits']//ul/li | //div[@id='benefits']//ol/li\",\n",
    "            \n",
    "            # Content after Benefits heading\n",
    "            \"//h2[contains(text(), 'Benefits')]/following-sibling::ul[1]/li | \" +\n",
    "            \"//h2[contains(text(), 'Benefits')]/following-sibling::div//ul/li\",\n",
    "            \n",
    "            # Markdown formatted lists\n",
    "            \"//div[contains(@class, 'markdown-options')]//ul/li\",\n",
    "            \n",
    "            # Any bullets within the main content\n",
    "            \"//main//ul/li[contains(@class, 'benefits') or ancestor::*[contains(@class, 'benefits')]]\"\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            try:\n",
    "                bullet_points = driver.find_elements(By.XPATH, selector)\n",
    "                if bullet_points:\n",
    "                    texts = [point.text.strip() for point in bullet_points if point.text.strip()]\n",
    "                    if texts:\n",
    "                        formatted_text = \"\\n\".join([f\"• {text}\" for text in texts])\n",
    "                        return formatted_text\n",
    "            except Exception as e:\n",
    "                print(f\"Bullet point selector {selector} failed: {e}\")\n",
    "        \n",
    "        # Try using JavaScript to extract content based on structure\n",
    "        script = \"\"\"\n",
    "        function extractBenefitsContent() {\n",
    "            // Approach 1: Find by ID or class\n",
    "            const benefitsSections = [\n",
    "                document.getElementById('benefits'),\n",
    "                ...Array.from(document.querySelectorAll('.benefits, .benefit-content, [data-section=\"benefits\"]'))\n",
    "            ].filter(Boolean);\n",
    "            \n",
    "            if (benefitsSections.length > 0) {\n",
    "                const section = benefitsSections[0];\n",
    "                \n",
    "                // Check for bullet points\n",
    "                const bullets = section.querySelectorAll('li');\n",
    "                if (bullets.length > 0) {\n",
    "                    return Array.from(bullets)\n",
    "                        .map(b => '• ' + b.innerText.trim())\n",
    "                        .join('\\\\n');\n",
    "                }\n",
    "                \n",
    "                // Otherwise return full text\n",
    "                return section.innerText.trim();\n",
    "            }\n",
    "            \n",
    "            // Approach 2: Find by looking for Benefits heading\n",
    "            const headings = Array.from(document.querySelectorAll('h1, h2, h3, h4, h5'));\n",
    "            const benefitsHeading = headings.find(h => \n",
    "                h.innerText.trim().toLowerCase().includes('benefits')\n",
    "            );\n",
    "            \n",
    "            if (benefitsHeading) {\n",
    "                // Get content between this heading and next heading\n",
    "                let content = '';\n",
    "                let currentNode = benefitsHeading.nextElementSibling;\n",
    "                \n",
    "                while (currentNode && \n",
    "                      !['H1', 'H2', 'H3', 'H4', 'H5'].includes(currentNode.tagName) &&\n",
    "                      !currentNode.innerText.includes('Eligibility') &&\n",
    "                      !currentNode.innerText.includes('Documents Required') &&\n",
    "                      !currentNode.innerText.includes('Application Process')) {\n",
    "                    \n",
    "                    if (currentNode.innerText.trim()) {\n",
    "                        content += currentNode.innerText.trim() + '\\\\n';\n",
    "                    }\n",
    "                    currentNode = currentNode.nextElementSibling;\n",
    "                }\n",
    "                \n",
    "                return content.trim();\n",
    "            }\n",
    "            \n",
    "            // Approach 3: Look for active tab content\n",
    "            const activeTabs = document.querySelectorAll('[role=\"tabpanel\"].active, .tab-content.active, .tab-pane.active');\n",
    "            for (const tab of activeTabs) {\n",
    "                if (tab.innerText.includes('Benefits') || \n",
    "                    tab.previousElementSibling?.innerText.includes('Benefits')) {\n",
    "                    return tab.innerText.trim();\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            // Approach 4: Look for content in data-slate-* tags (as seen in your images)\n",
    "            const slateElements = document.querySelectorAll('[data-slate-string=\"true\"]');\n",
    "            if (slateElements.length > 0) {\n",
    "                // Check if any parent has \"Benefits\" text\n",
    "                for (const elem of slateElements) {\n",
    "                    let parent = elem.parentElement;\n",
    "                    while (parent && parent !== document.body) {\n",
    "                        if (parent.innerText.toLowerCase().includes('benefits')) {\n",
    "                            // Found a slate element inside Benefits section\n",
    "                            const parentSection = parent.closest('div[data-slate-node=\"element\"]');\n",
    "                            if (parentSection) {\n",
    "                                return parentSection.innerText;\n",
    "                            }\n",
    "                        }\n",
    "                        parent = parent.parentElement;\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return '';\n",
    "        }\n",
    "        \n",
    "        return extractBenefitsContent();\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            js_result = driver.execute_script(script)\n",
    "            if js_result and len(js_result) > 10:\n",
    "                return js_result\n",
    "        except Exception as e:\n",
    "            print(f\"JavaScript extraction failed: {e}\")\n",
    "        \n",
    "        # Last resort: Find any content that might be benefits\n",
    "        fallback_script = \"\"\"\n",
    "        function findPotentialBenefits() {\n",
    "            // Look for paragraphs with key benefit phrases\n",
    "            const benefitPhrases = ['amount', 'compensation', 'grant', 'stipend', 'financial', 'assistance', 'support', 'paid', 'receive'];\n",
    "            \n",
    "            const paragraphs = document.querySelectorAll('p');\n",
    "            let benefitsText = '';\n",
    "            \n",
    "            for (const p of paragraphs) {\n",
    "                const text = p.innerText.toLowerCase();\n",
    "                if (benefitPhrases.some(phrase => text.includes(phrase))) {\n",
    "                    benefitsText += '• ' + p.innerText.trim() + '\\\\n';\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            // If we found something\n",
    "            if (benefitsText) return benefitsText;\n",
    "            \n",
    "            // Try finding main content\n",
    "            const mainContent = document.querySelector('main') || document.querySelector('.content') || document.body;\n",
    "            const mainText = mainContent.innerText;\n",
    "            \n",
    "            // Try to find Benefits section within main content\n",
    "            const benefitsIndex = mainText.indexOf('Benefits');\n",
    "            if (benefitsIndex >= 0) {\n",
    "                // Look for next section heading\n",
    "                const nextSectionIndex = Math.min(\n",
    "                    ...[\n",
    "                        mainText.indexOf('Eligibility', benefitsIndex),\n",
    "                        mainText.indexOf('Documents Required', benefitsIndex),\n",
    "                        mainText.indexOf('Application Process', benefitsIndex)\n",
    "                    ].filter(idx => idx > 0)\n",
    "                );\n",
    "                \n",
    "                if (nextSectionIndex > benefitsIndex) {\n",
    "                    return mainText.substring(benefitsIndex, nextSectionIndex).trim();\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return '';\n",
    "        }\n",
    "        \n",
    "        return findPotentialBenefits();\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            fallback_result = driver.execute_script(fallback_script)\n",
    "            if fallback_result and len(fallback_result) > 10:\n",
    "                return fallback_result\n",
    "        except Exception as e:\n",
    "            print(f\"Fallback extraction failed: {e}\")\n",
    "        \n",
    "        return \"No benefits information found\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting benefits: {e}\")\n",
    "        return \"Error: \" + str(e)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process scheme links and extract benefits information.\n",
    "    \"\"\"\n",
    "    # Load the scheme details from the input file\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Input file {INPUT_FILE} not found.\")\n",
    "        return\n",
    "        \n",
    "    schemes_df = pd.read_excel(INPUT_FILE)\n",
    "    \n",
    "    # Check if the input file has the required columns\n",
    "    required_columns = ['scheme_name', 'scheme_link']\n",
    "    if not all(col in schemes_df.columns for col in required_columns):\n",
    "        print(f\"Input file missing required columns. Required: {required_columns}\")\n",
    "        return\n",
    "    \n",
    "    # Add a benefits_text column if it doesn't exist\n",
    "    if 'benefits_text' not in schemes_df.columns:\n",
    "        schemes_df['benefits_text'] = \"\"\n",
    "    \n",
    "    # Filter only rows without benefits information\n",
    "    no_benefits_df = schemes_df[schemes_df['benefits_text'].isna() | (schemes_df['benefits_text'] == \"\")]\n",
    "    has_benefits_df = schemes_df[~(schemes_df['benefits_text'].isna() | (schemes_df['benefits_text'] == \"\"))]\n",
    "    \n",
    "    print(f\"Found {len(no_benefits_df)} schemes without benefits information\")\n",
    "    print(f\"{len(has_benefits_df)} schemes already have benefits information\")\n",
    "    \n",
    "    if len(no_benefits_df) == 0:\n",
    "        print(\"No schemes need processing. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the WebDriver\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    # Process schemes without benefits\n",
    "    updated_rows = []\n",
    "    total_to_process = len(no_benefits_df)\n",
    "    \n",
    "    for idx, row in no_benefits_df.iterrows():\n",
    "        scheme_name = row['scheme_name']\n",
    "        scheme_link = row['scheme_link']\n",
    "        \n",
    "        # Skip if URL is invalid or empty\n",
    "        if not scheme_link or not scheme_link.startswith('http'):\n",
    "            print(f\"Skipping invalid URL for: {scheme_name}\")\n",
    "            updated_row = row.copy()\n",
    "            updated_row['benefits_text'] = \"Error: Invalid URL\"\n",
    "            updated_rows.append(updated_row)\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n[{idx+1}/{total_to_process}] Processing: {scheme_name}\")\n",
    "        print(f\"URL: {scheme_link}\")\n",
    "        \n",
    "        # Try to extract benefits with retries and exponential backoff\n",
    "        benefits_text = None\n",
    "        retries = 0\n",
    "        backoff_time = 2  # start with 2 seconds\n",
    "        \n",
    "        while benefits_text is None and retries < MAX_RETRIES:\n",
    "            try:\n",
    "                benefits_text = extract_benefits_section(driver, scheme_link)\n",
    "                \n",
    "                # Check if no benefits were found\n",
    "                if not benefits_text or benefits_text == \"No benefits information found\":\n",
    "                    print(f\"Attempt {retries+1}: Could not find benefits\")\n",
    "                    retries += 1\n",
    "                    \n",
    "                    if retries < MAX_RETRIES:\n",
    "                        print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                        time.sleep(backoff_time)\n",
    "                        backoff_time *= 1.5  # Exponential backoff\n",
    "                        benefits_text = None  # Reset to trigger retry\n",
    "                    else:\n",
    "                        benefits_text = \"No benefits information found after multiple attempts\"\n",
    "                else:\n",
    "                    print(f\"Successfully extracted benefits: {len(benefits_text)} characters\")\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {retries+1} failed: {e}\")\n",
    "                retries += 1\n",
    "                \n",
    "                if retries < MAX_RETRIES:\n",
    "                    print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                    time.sleep(backoff_time)\n",
    "                    backoff_time *= 1.5  # Exponential backoff\n",
    "                else:\n",
    "                    benefits_text = f\"Error: Failed to extract benefits after {MAX_RETRIES} attempts\"\n",
    "        \n",
    "        # Update the row\n",
    "        updated_row = row.copy()\n",
    "        updated_row['benefits_text'] = benefits_text\n",
    "        updated_rows.append(updated_row)\n",
    "        \n",
    "        # Save checkpoint every 5 schemes\n",
    "        if (idx + 1) % 5 == 0 or (idx + 1) == total_to_process:\n",
    "            # Create temporary DataFrame with processed rows\n",
    "            processed_df = pd.DataFrame(updated_rows)\n",
    "            # Merge with schemes that already have benefits\n",
    "            temp_df = pd.concat([has_benefits_df, processed_df])\n",
    "            temp_df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "            print(f\"Checkpoint saved. Processed {idx+1}/{total_to_process} schemes without benefits.\")\n",
    "            \n",
    "        # Add a random delay to avoid detection (longer delay for more robustness)\n",
    "        delay_time = 2 + random.random() * 3\n",
    "        print(f\"Waiting {delay_time:.2f} seconds before next scheme...\")\n",
    "        time.sleep(delay_time)\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "    \n",
    "    # Combine updated rows with rows that already had benefits\n",
    "    final_df = pd.concat([has_benefits_df, pd.DataFrame(updated_rows)])\n",
    "    \n",
    "    # Sort by original index to maintain the original order if available\n",
    "    if 'original_index' in final_df.columns:\n",
    "        final_df = final_df.sort_values('original_index')\n",
    "    \n",
    "    # Save final results\n",
    "    final_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nExtraction complete! Updated {len(updated_rows)} schemes.\")\n",
    "    print(f\"Results saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292304f2-5242-4b98-949c-7af602ae9d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the Eligibility section of each scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939f428-4fb4-4a85-8744-fdef9010f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Load existing Excel file with scheme names and links\n",
    "df = pd.read_excel(\"myscheme_details_with_benefits.xlsx\")\n",
    "\n",
    "# Create a new column for eligibility information if it doesn't exist\n",
    "if 'eligibility_text' not in df.columns:\n",
    "    df['eligibility_text'] = \"\"\n",
    "\n",
    "# Function to extract eligibility information from a scheme page\n",
    "def extract_eligibility(url):\n",
    "    try:\n",
    "        # Modify URL to directly access the eligibility section\n",
    "        base_url = url.split('#')[0] if '#' in url else url\n",
    "        eligibility_url = base_url\n",
    "        \n",
    "        # If URL doesn't end with /eligibility, try appending it\n",
    "        if not eligibility_url.endswith(\"/eligibility\"):\n",
    "            # Check for fadsp pattern shown in your example\n",
    "            if \"fadsp\" in eligibility_url or \"schemes\" in eligibility_url:\n",
    "                if not eligibility_url.endswith('/'):\n",
    "                    eligibility_url += '/'\n",
    "                eligibility_url += \"#eligibility\"\n",
    "        \n",
    "        # Send HTTP request with proper headers\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(eligibility_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Try to find eligibility section in various formats\n",
    "        eligibility_section = None\n",
    "        \n",
    "        # Method 1: Find the section by heading\n",
    "        for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "            if \"eligibility\" in heading.text.strip().lower():\n",
    "                eligibility_section = heading\n",
    "                break\n",
    "        \n",
    "        eligibility_points = []\n",
    "        \n",
    "        if eligibility_section:\n",
    "            # Get the parent section that might contain all eligibility info\n",
    "            parent_section = eligibility_section.parent\n",
    "            \n",
    "            # Method 1: Try to find list items within the section\n",
    "            if parent_section:\n",
    "                list_items = parent_section.find_all('li')\n",
    "                if list_items:\n",
    "                    eligibility_points = [item.text.strip() for item in list_items]\n",
    "            \n",
    "            # Method 2: Look for paragraphs with Roman numeral patterns like (i), (ii), etc.\n",
    "            if not eligibility_points:\n",
    "                roman_pattern = re.compile(r'\\((i{1,3}|iv|v|vi{1,3}|ix|x)\\)')\n",
    "                \n",
    "                # Check all paragraphs after the eligibility heading\n",
    "                next_elements = []\n",
    "                current = eligibility_section.next_sibling\n",
    "                \n",
    "                # Collect siblings until we hit another heading or a maximum number of elements\n",
    "                while current and len(next_elements) < 20:\n",
    "                    if current.name and current.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                        break\n",
    "                    if current.name in ['p', 'div'] and current.text.strip():\n",
    "                        next_elements.append(current)\n",
    "                    current = current.next_sibling\n",
    "                \n",
    "                # Check collected elements for Roman numeral patterns\n",
    "                for element in next_elements:\n",
    "                    if roman_pattern.search(element.text):\n",
    "                        eligibility_points.append(element.text.strip())\n",
    "                    elif element.text.strip() and len(element.text.strip()) > 20:  # Arbitrary minimum length for meaningful content\n",
    "                        eligibility_points.append(element.text.strip())\n",
    "            \n",
    "            # Method 3: Look for standard numbered paragraphs (1., 2., etc.)\n",
    "            if not eligibility_points:\n",
    "                number_pattern = re.compile(r'^\\d+\\.')\n",
    "                next_elements = eligibility_section.find_next_siblings(['p', 'div'])\n",
    "                \n",
    "                for element in next_elements:\n",
    "                    text = element.text.strip()\n",
    "                    if number_pattern.search(text):\n",
    "                        eligibility_points.append(text)\n",
    "                    elif text and len(text) > 20:  # Check for meaningful content\n",
    "                        eligibility_points.append(text)\n",
    "        \n",
    "        # If still no eligibility points, try a broader search\n",
    "        if not eligibility_points:\n",
    "            # Look for any section that might contain eligibility info\n",
    "            for section in soup.find_all(['div', 'section']):\n",
    "                if \"eligibility\" in section.get('id', '').lower() or \"eligibility\" in section.get('class', [''])[0].lower() if section.get('class') else False:\n",
    "                    # Check all paragraphs within this section\n",
    "                    paragraphs = section.find_all(['p', 'div'])\n",
    "                    for p in paragraphs:\n",
    "                        if p.text.strip() and len(p.text.strip()) > 20:  # Check for meaningful content\n",
    "                            eligibility_points.append(p.text.strip())\n",
    "        \n",
    "        # If still nothing, look for any paragraph that has keywords indicating eligibility criteria\n",
    "        if not eligibility_points:\n",
    "            eligibility_keywords = [\n",
    "                \"eligible to apply\", \"can apply\", \"applicant should\", \"applicant must\",\n",
    "                \"requirements for\", \"criteria for\", \"qualification\", \"who can apply\"\n",
    "            ]\n",
    "            \n",
    "            for p in soup.find_all(['p', 'div', 'span']):\n",
    "                text = p.text.strip()\n",
    "                if any(keyword in text.lower() for keyword in eligibility_keywords) and len(text) > 20:\n",
    "                    eligibility_points.append(text)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        unique_eligibility_points = []\n",
    "        seen_points = set()\n",
    "        for point in eligibility_points:\n",
    "            # Clean the point text to normalize it for comparison\n",
    "            # Remove numbering and special characters for comparison\n",
    "            cleaned_text = re.sub(r'^\\d+\\.\\s*|\\([ivx]+\\)\\s*', '', point).strip()\n",
    "            # Only add if we haven't seen this text before\n",
    "            if cleaned_text not in seen_points and cleaned_text:\n",
    "                seen_points.add(cleaned_text)\n",
    "                unique_eligibility_points.append(point)\n",
    "        \n",
    "        # Join all points into a single text with consistent numbering\n",
    "        if unique_eligibility_points:\n",
    "            # Clean up the points and ensure they have numbers\n",
    "            cleaned_points = []\n",
    "            for i, point in enumerate(unique_eligibility_points):\n",
    "                # Remove existing numbers if they exist and add consistent numbering\n",
    "                # This handles both numeric (1., 2.) and Roman numeral ((i), (ii)) formats\n",
    "                point = re.sub(r'^\\d+\\.\\s*|\\([ivx]+\\)\\s*', '', point).strip()\n",
    "                cleaned_points.append(f\"{i+1}. {point}\")\n",
    "            \n",
    "            return \"\\n\".join(cleaned_points)\n",
    "        else:\n",
    "            return \"No eligibility information found.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error extracting eligibility: {e}\"\n",
    "\n",
    "# Process each scheme in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    if pd.notna(row['scheme_link']):\n",
    "        print(f\"Processing {row['scheme_name']}...\")\n",
    "        \n",
    "        # Extract eligibility information\n",
    "        eligibility_info = extract_eligibility(row['scheme_link'])\n",
    "        \n",
    "        # Update the dataframe\n",
    "        df.at[index, 'eligibility_text'] = eligibility_info\n",
    "        \n",
    "        # Pause to avoid overloading the server\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        df.at[index, 'eligibility_text'] = \"No link available\"\n",
    "\n",
    "# Save the updated dataframe to Excel\n",
    "df.to_excel(\"myscheme_details_with_eligibility.xlsx\", index=False)\n",
    "print(\"Extraction completed and saved to myscheme_details_with_eligibility.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f361f-e797-4b3c-ab60-54aaaca3b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the Exclusion section of each scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdaf606-be14-4046-b35f-55111a223fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # for progress reporting\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# File paths\n",
    "INPUT_FILE = \"myscheme_details_with_eligibility.xlsx\"  # Your existing file with scheme links\n",
    "OUTPUT_FILE = \"myscheme_exlusions.xlsx\"                # New file for scheme details (exclusions)\n",
    "CHECKPOINT_FILE = \"exlusions_checkpoint.xlsx\"          # Checkpoint file for progress tracking\n",
    "\n",
    "# Maximum number of retries for failed requests\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Configure and return a headless Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def load_input():\n",
    "    \"\"\"\n",
    "    Load the input Excel file with scheme details and ensure an 'original_index' column exists.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Error: {INPUT_FILE} not found. Run the initial scraper first.\")\n",
    "        return None\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    if \"original_index\" not in df.columns:\n",
    "        df[\"original_index\"] = df.index\n",
    "    return df\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"\n",
    "    Load the checkpoint file to track progress.\n",
    "    Returns a DataFrame and a set of processed scheme links.\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        details_df = pd.read_excel(CHECKPOINT_FILE)\n",
    "        if not details_df.empty and 'scheme_link' in details_df.columns:\n",
    "            processed_links = set(details_df[\"scheme_link\"])\n",
    "            return details_df, processed_links\n",
    "    columns = [\"scheme_name\", \"scheme_link\", \"details_text\", \"original_index\", \"exclusions_text\"]\n",
    "    return pd.DataFrame(columns=columns), set()\n",
    "\n",
    "def save_checkpoint(df):\n",
    "    \"\"\"\n",
    "    Save current progress to the checkpoint file.\n",
    "    \"\"\"\n",
    "    df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "    print(f\"Checkpoint saved with {len(df)} schemes.\")\n",
    "\n",
    "def extract_exclusions_section(driver, url):\n",
    "    \"\"\"\n",
    "    Extract the 'Exclusions' section content from a scheme page.\n",
    "    Attempts to capture all text in the Exclusions container (including numbered list items and additional notes).\n",
    "    If not found, returns a default message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Wait for the page to load (up to 15 seconds)\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        time.sleep(2)  # Additional delay for JavaScript rendering\n",
    "\n",
    "        # Attempt 1: Locate the Exclusions section by its ID\n",
    "        try:\n",
    "            exclusions_container = driver.find_element(By.ID, \"exclusions\")\n",
    "        except NoSuchElementException:\n",
    "            exclusions_container = None\n",
    "\n",
    "        # Attempt 2: If not found by ID, try finding a header with \"Exclusions\"\n",
    "        if not exclusions_container:\n",
    "            headers = driver.find_elements(\n",
    "                By.XPATH, \n",
    "                \"//h2[contains(translate(text(),'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'exclusions')]\"\n",
    "            )\n",
    "            if headers:\n",
    "                exclusions_container = headers[0].find_element(By.XPATH, \"ancestor::div[1]\")\n",
    "        \n",
    "        if exclusions_container:\n",
    "            # Instead of extracting only the ordered list, get the full text content of the container.\n",
    "            full_text = exclusions_container.text.strip()\n",
    "            if full_text:\n",
    "                return full_text\n",
    "        \n",
    "        # If no container or text is found, return a default message.\n",
    "        return \"Section Not Available against this scheme\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting Exclusions section from {url}: {e}\")\n",
    "        return \"Section Not Available against this scheme\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process scheme links and extract the Exclusions section.\n",
    "    It loads the input file, processes only those schemes that have not yet been processed,\n",
    "    extracts the Exclusions section, and saves the data to an output Excel file.\n",
    "    \"\"\"\n",
    "    input_df = load_input()\n",
    "    if input_df is None:\n",
    "        return\n",
    "    print(f\"Loaded {len(input_df)} schemes from {INPUT_FILE}.\")\n",
    "    \n",
    "    checkpoint_df, processed_links = load_checkpoint()\n",
    "    print(f\"Already processed {len(processed_links)} scheme links from checkpoint.\")\n",
    "    \n",
    "    # Create a new column for exclusions if it doesn't exist.\n",
    "    if \"exclusions_text\" not in input_df.columns:\n",
    "        input_df[\"exclusions_text\"] = \"\"\n",
    "    \n",
    "    # Filter rows without exclusions information.\n",
    "    no_exclusions_df = input_df[input_df[\"exclusions_text\"].isna() | (input_df[\"exclusions_text\"] == \"\")]\n",
    "    has_exclusions_df = input_df[~(input_df[\"exclusions_text\"].isna() | (input_df[\"exclusions_text\"] == \"\"))]\n",
    "    \n",
    "    print(f\"Found {len(no_exclusions_df)} schemes without exclusions information\")\n",
    "    print(f\"{len(has_exclusions_df)} schemes already have exclusions information\")\n",
    "    \n",
    "    if len(no_exclusions_df) == 0:\n",
    "        print(\"No schemes need processing. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    updated_rows = []\n",
    "    total_to_process = len(no_exclusions_df)\n",
    "    \n",
    "    for idx, row in tqdm(no_exclusions_df.iterrows(), total=total_to_process, desc=\"Processing schemes for Exclusions\"):\n",
    "        scheme_name = row[\"scheme_name\"]\n",
    "        scheme_link = row[\"scheme_link\"]\n",
    "        \n",
    "        # Skip if URL is invalid\n",
    "        if not scheme_link or not scheme_link.startswith(\"http\"):\n",
    "            print(f\"Skipping invalid URL for: {scheme_name}\")\n",
    "            updated_row = row.copy()\n",
    "            updated_row[\"exclusions_text\"] = \"Error: Invalid URL\"\n",
    "            updated_rows.append(updated_row)\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing scheme: {scheme_name}\")\n",
    "        print(f\"URL: {scheme_link}\")\n",
    "        \n",
    "        exclusions_text = None\n",
    "        retries = 0\n",
    "        backoff_time = 2  # starting delay in seconds\n",
    "        \n",
    "        while exclusions_text is None and retries < MAX_RETRIES:\n",
    "            try:\n",
    "                exclusions_text = extract_exclusions_section(driver, scheme_link)\n",
    "                if not exclusions_text or len(exclusions_text) < 10:\n",
    "                    print(f\"Attempt {retries+1}: Insufficient exclusions content for {scheme_name}\")\n",
    "                    retries += 1\n",
    "                    if retries < MAX_RETRIES:\n",
    "                        print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                        time.sleep(backoff_time)\n",
    "                        backoff_time *= 1.5\n",
    "                        exclusions_text = None  # Reset for retry\n",
    "                    else:\n",
    "                        exclusions_text = \"Section Not Available against this scheme\"\n",
    "                else:\n",
    "                    print(f\"Successfully extracted exclusions: {len(exclusions_text)} characters\")\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {retries+1} failed for {scheme_name}: {e}\")\n",
    "                retries += 1\n",
    "                if retries < MAX_RETRIES:\n",
    "                    print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                    time.sleep(backoff_time)\n",
    "                    backoff_time *= 1.5\n",
    "                else:\n",
    "                    exclusions_text = \"Section Not Available against this scheme\"\n",
    "        \n",
    "        updated_row = row.copy()\n",
    "        updated_row[\"exclusions_text\"] = exclusions_text\n",
    "        updated_rows.append(updated_row)\n",
    "        \n",
    "        # Save checkpoint every 5 schemes or at the end\n",
    "        if (idx + 1) % 5 == 0 or (idx + 1) == total_to_process:\n",
    "            processed_df = pd.DataFrame(updated_rows)\n",
    "            temp_df = pd.concat([has_exclusions_df, processed_df], ignore_index=True)\n",
    "            temp_df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "            print(f\"Checkpoint saved. Processed {idx+1}/{total_to_process} schemes without exclusions.\")\n",
    "        \n",
    "        delay_time = 2 + random.random() * 3\n",
    "        print(f\"Waiting {delay_time:.2f} seconds before next scheme...\")\n",
    "        time.sleep(delay_time)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    final_df = pd.concat([has_exclusions_df, pd.DataFrame(updated_rows)], ignore_index=True)\n",
    "    if \"original_index\" in final_df.columns:\n",
    "        final_df = final_df.sort_values(\"original_index\")\n",
    "    final_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nExtraction complete! Updated {len(updated_rows)} schemes.\")\n",
    "    print(f\"Results saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02eb7a-e7da-4f6c-a0cd-9819317cf1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the Application mode of each scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafc677f-df54-408b-9282-bd443aa20953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress reporting\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# File paths (adjust these paths as needed)\n",
    "INPUT_FILE = \"myscheme_exlusions.xlsx\"  # Existing file with scheme_name, scheme_link, etc.\n",
    "OUTPUT_FILE = \"myscheme_application_mode.xlsx\"         # Final output file with Application Mode info\n",
    "CHECKPOINT_FILE = \"application_mode_checkpoint.xlsx\"    # Checkpoint file for progress\n",
    "\n",
    "# Maximum number of retries for failed extraction attempts\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Configure and return a headless Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def load_input():\n",
    "    \"\"\"\n",
    "    Load the input Excel file with scheme details and ensure an 'original_index' column exists.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Error: {INPUT_FILE} not found. Run the initial scraper first.\")\n",
    "        return None\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    if \"original_index\" not in df.columns:\n",
    "        df[\"original_index\"] = df.index\n",
    "    return df\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"\n",
    "    Load the checkpoint file to track progress.\n",
    "    Returns a DataFrame and a set of processed scheme links.\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        df = pd.read_excel(CHECKPOINT_FILE)\n",
    "        if not df.empty and 'scheme_link' in df.columns:\n",
    "            processed_links = set(df[\"scheme_link\"])\n",
    "            return df, processed_links\n",
    "    # If file doesn't exist, start fresh with desired columns.\n",
    "    columns = [\"scheme_name\", \"scheme_link\", \"page\", \"application_mode\", \"original_index\"]\n",
    "    return pd.DataFrame(columns=columns), set()\n",
    "\n",
    "def save_checkpoint(df):\n",
    "    \"\"\"\n",
    "    Save current progress to the checkpoint file.\n",
    "    \"\"\"\n",
    "    df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "    print(f\"Checkpoint saved with {len(df)} schemes.\")\n",
    "\n",
    "def extract_application_mode(driver, url):\n",
    "    \"\"\"\n",
    "    Extract the Application Mode (Online, Offline, or both) from a scheme page.\n",
    "    \n",
    "    The function loads the page (using the original URL; note that the fragment part, e.g., \"#application-process\",\n",
    "    is not necessary to load the content), scrolls to the \"Application Process\" section, and then searches for mode indicators.\n",
    "    \n",
    "    Returns a string listing available modes (e.g., \"Online\", \"Offline\", or \"Offline, Online\").\n",
    "    If no mode is found, returns \"No mode information found\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        time.sleep(2)  # Additional delay for JS to render\n",
    "\n",
    "        # Scroll to the Application Process section using its ID\n",
    "        try:\n",
    "            app_section = driver.find_element(By.ID, \"application-process\")\n",
    "        except NoSuchElementException:\n",
    "            # If the section is not explicitly marked with an id, try finding a heading\n",
    "            headings = driver.find_elements(By.XPATH, \"//h2[contains(translate(text(),'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'application process')]\")\n",
    "            if headings:\n",
    "                app_section = headings[0].find_element(By.XPATH, \"ancestor::div[1]\")\n",
    "            else:\n",
    "                return \"Section Not Available against this scheme\"\n",
    "\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", app_section)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Now, within the application process section, look for span elements that contain mode names.\n",
    "        # We assume the mode names will be \"Online\" or \"Offline\" (case-insensitive).\n",
    "        mode_elements = app_section.find_elements(By.XPATH, \".//span\")\n",
    "        modes_found = []\n",
    "        for elem in mode_elements:\n",
    "            text = elem.text.strip()\n",
    "            if text.lower() in [\"online\", \"offline\"]:\n",
    "                modes_found.append(text)\n",
    "        \n",
    "        if modes_found:\n",
    "            # Remove duplicates and sort (so order is consistent)\n",
    "            unique_modes = sorted(set(modes_found))\n",
    "            return \", \".join(unique_modes)\n",
    "        else:\n",
    "            return \"No mode information found\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting application mode from {url}: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process scheme links and extract Application Mode information.\n",
    "    It loads the input file, processes only those schemes that have not yet been processed,\n",
    "    extracts the Application Mode from the Application Process section, and saves the data to an output Excel file.\n",
    "    \"\"\"\n",
    "    input_df = load_input()\n",
    "    if input_df is None:\n",
    "        return\n",
    "    print(f\"Loaded {len(input_df)} schemes from {INPUT_FILE}.\")\n",
    "\n",
    "    checkpoint_df, processed_links = load_checkpoint()\n",
    "    print(f\"Already processed {len(processed_links)} scheme links from checkpoint.\")\n",
    "\n",
    "    # Create a new column for application_mode if it doesn't exist.\n",
    "    if \"application_mode\" not in input_df.columns:\n",
    "        input_df[\"application_mode\"] = \"\"\n",
    "\n",
    "    # Filter rows without application mode information.\n",
    "    no_mode_df = input_df[input_df[\"application_mode\"].isna() | (input_df[\"application_mode\"] == \"\")]\n",
    "    has_mode_df = input_df[~(input_df[\"application_mode\"].isna() | (input_df[\"application_mode\"] == \"\"))]\n",
    "\n",
    "    print(f\"Found {len(no_mode_df)} schemes without application mode information\")\n",
    "    print(f\"{len(has_mode_df)} schemes already have application mode information\")\n",
    "\n",
    "    if len(no_mode_df) == 0:\n",
    "        print(\"No schemes need processing. Exiting.\")\n",
    "        return\n",
    "\n",
    "    driver = setup_driver()\n",
    "    updated_rows = []\n",
    "    total_to_process = len(no_mode_df)\n",
    "\n",
    "    for idx, row in tqdm(no_mode_df.iterrows(), total=total_to_process, desc=\"Processing schemes for Application Mode\"):\n",
    "        scheme_name = row[\"scheme_name\"]\n",
    "        scheme_link = row[\"scheme_link\"]\n",
    "\n",
    "        # Skip if URL is invalid\n",
    "        if not scheme_link or not scheme_link.startswith(\"http\"):\n",
    "            print(f\"Skipping invalid URL for: {scheme_name}\")\n",
    "            updated_row = row.copy()\n",
    "            updated_row[\"application_mode\"] = \"Error: Invalid URL\"\n",
    "            updated_rows.append(updated_row)\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing scheme: {scheme_name}\")\n",
    "        print(f\"URL: {scheme_link}\")\n",
    "\n",
    "        mode_text = None\n",
    "        retries = 0\n",
    "        backoff_time = 2  # Starting delay in seconds\n",
    "\n",
    "        while mode_text is None and retries < MAX_RETRIES:\n",
    "            try:\n",
    "                mode_text = extract_application_mode(driver, scheme_link)\n",
    "                if not mode_text or len(mode_text) < 3 or mode_text == \"No mode information found\":\n",
    "                    print(f\"Attempt {retries+1}: Insufficient application mode info for {scheme_name}\")\n",
    "                    retries += 1\n",
    "                    if retries < MAX_RETRIES:\n",
    "                        print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                        time.sleep(backoff_time)\n",
    "                        backoff_time *= 1.5\n",
    "                        mode_text = None  # Reset for retry\n",
    "                    else:\n",
    "                        mode_text = \"Section Not Available against this scheme\"\n",
    "                else:\n",
    "                    print(f\"Successfully extracted application mode: {mode_text}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {retries+1} failed for {scheme_name}: {e}\")\n",
    "                retries += 1\n",
    "                if retries < MAX_RETRIES:\n",
    "                    print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                    time.sleep(backoff_time)\n",
    "                    backoff_time *= 1.5\n",
    "                else:\n",
    "                    mode_text = \"Section Not Available against this scheme\"\n",
    "\n",
    "        updated_row = row.copy()\n",
    "        updated_row[\"application_mode\"] = mode_text\n",
    "        updated_rows.append(updated_row)\n",
    "\n",
    "        # Save checkpoint every 5 schemes or at the end\n",
    "        if (idx + 1) % 5 == 0 or (idx + 1) == total_to_process:\n",
    "            processed_df = pd.DataFrame(updated_rows)\n",
    "            temp_df = pd.concat([has_mode_df, processed_df], ignore_index=True)\n",
    "            temp_df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "            print(f\"Checkpoint saved. Processed {idx+1}/{total_to_process} schemes without application mode.\")\n",
    "        \n",
    "        delay_time = 2 + random.random() * 3\n",
    "        print(f\"Waiting {delay_time:.2f} seconds before next scheme...\")\n",
    "        time.sleep(delay_time)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    final_df = pd.concat([has_mode_df, pd.DataFrame(updated_rows)], ignore_index=True)\n",
    "    if \"original_index\" in final_df.columns:\n",
    "        final_df = final_df.sort_values(\"original_index\")\n",
    "    final_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nExtraction complete! Updated {len(updated_rows)} schemes.\")\n",
    "    print(f\"Results saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ff4666-92df-4f7b-b5f9-633a9e1d4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the Application process section of each scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cddf34-dd76-44ca-8122-6b119eaa0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # for progress reporting\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# File paths\n",
    "INPUT_FILE = \"myscheme_application_mode.xlsx\"  # Your input file with scheme links, etc.\n",
    "OUTPUT_FILE = \"myscheme_application_process.xlsx\"       # Final output file with Application Process details\n",
    "CHECKPOINT_FILE = \"application_process_checkpoint.xlsx\"  # Checkpoint file to resume progress\n",
    "\n",
    "# Maximum number of retries for failed extraction attempts\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Configure and return a headless Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def load_input():\n",
    "    \"\"\"\n",
    "    Load the input Excel file with scheme details and ensure an 'original_index' column exists.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Error: {INPUT_FILE} not found. Run the initial scraper first.\")\n",
    "        return None\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    if \"original_index\" not in df.columns:\n",
    "        df[\"original_index\"] = df.index\n",
    "    return df\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"\n",
    "    Load the checkpoint file to track progress.\n",
    "    Returns a DataFrame and a set of processed scheme links.\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        df = pd.read_excel(CHECKPOINT_FILE)\n",
    "        if not df.empty and 'scheme_link' in df.columns:\n",
    "            processed_links = set(df[\"scheme_link\"])\n",
    "            return df, processed_links\n",
    "    columns = [\"scheme_name\", \"scheme_link\", \"page\", \"application_steps_offline\", \n",
    "               \"application_notes_offline\", \"application_steps_online\", \n",
    "               \"application_notes_online\", \"original_index\"]\n",
    "    return pd.DataFrame(columns=columns), set()\n",
    "\n",
    "def save_checkpoint(df):\n",
    "    \"\"\"\n",
    "    Save current progress to the checkpoint file.\n",
    "    \"\"\"\n",
    "    df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "    print(f\"Checkpoint saved with {len(df)} schemes.\")\n",
    "\n",
    "def clean_application_text(text):\n",
    "    \"\"\"\n",
    "    Clean the application process text by removing unwanted header text and tabs.\n",
    "    \"\"\"\n",
    "    # Remove \"Application Process\" header if present\n",
    "    text = re.sub(r\"^Application Process\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove \"Offline\" or \"Online\" tabs if present\n",
    "    text = re.sub(r\"^(Offline|Online)\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Also handle case where they might be in the middle of text due to newlines\n",
    "    text = re.sub(r\"\\n(Offline|Online)\\s*\", \"\\n\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove duplicate \"Application Process\" subheadings\n",
    "    text = re.sub(r\"\\nApplication Process\\s*\", \"\\n\", text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_content_from_current_tab(driver, container):\n",
    "    \"\"\"\n",
    "    Extract application process content from the currently active tab.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to get content from paragraphs\n",
    "        paragraphs = container.find_elements(By.TAG_NAME, \"p\")\n",
    "        if paragraphs:\n",
    "            content_text = \"\\n\".join([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "        else:\n",
    "            # Try to get numbered list items\n",
    "            list_items = container.find_elements(By.XPATH, \".//ol/li | .//ul/li\")\n",
    "            if list_items:\n",
    "                content_text = \"\\n\".join([f\"{i+1}. {item.text.strip()}\" for i, item in enumerate(list_items) if item.text.strip()])\n",
    "            else:\n",
    "                # Fall back to all text\n",
    "                content_text = container.text.strip()\n",
    "                \n",
    "        # Clean the text\n",
    "        content_text = clean_application_text(content_text)\n",
    "        \n",
    "        # Better parsing of steps and notes\n",
    "        steps = []\n",
    "        notes = []\n",
    "        \n",
    "        # Split by lines to process step by step\n",
    "        lines = content_text.split('\\n')\n",
    "        current_section = \"steps\"  # Default section\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            # Check if this line is a note\n",
    "            if line.lower().startswith(\"note:\") or line.lower().startswith(\"note \"):\n",
    "                current_section = \"notes\"\n",
    "                notes.append(line)\n",
    "            # Check if this is a step indicator\n",
    "            elif line.lower().startswith(\"step\") or re.match(r\"^\\d+\\.\", line):\n",
    "                current_section = \"steps\"\n",
    "                steps.append(line)\n",
    "            # Otherwise add to current section\n",
    "            else:\n",
    "                if current_section == \"notes\":\n",
    "                    notes.append(line)\n",
    "                else:\n",
    "                    steps.append(line)\n",
    "        \n",
    "        steps_text = \"\\n\".join(steps).strip()\n",
    "        notes_text = \"\\n\".join(notes).strip()\n",
    "        \n",
    "        if not notes_text:\n",
    "            notes_text = \"No specific notes found\"\n",
    "            \n",
    "        return {\"steps\": steps_text, \"notes\": notes_text}\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content from tab: {e}\")\n",
    "        return {\"steps\": \"Error extracting content\", \"notes\": \"Error extracting content\"}\n",
    "\n",
    "def extract_application_process_section(driver, url):\n",
    "    \"\"\"\n",
    "    Extract the 'Application Process' section from a scheme page,\n",
    "    including both online and offline modes if available.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        time.sleep(3)  # Allow extra time for JavaScript rendering\n",
    "        \n",
    "        # Attempt to locate the Application Process section by its ID.\n",
    "        try:\n",
    "            app_proc_container = driver.find_element(By.ID, \"application-process\")\n",
    "        except NoSuchElementException:\n",
    "            # If not found by ID, try finding a heading with \"Application Process\"\n",
    "            headings = driver.find_elements(By.XPATH, \n",
    "                \"//h2[contains(translate(text(),'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'application process')]\")\n",
    "            if headings:\n",
    "                app_proc_container = headings[0].find_element(By.XPATH, \"ancestor::div[2]\")\n",
    "            else:\n",
    "                # Try to find content in div with markdown content\n",
    "                try:\n",
    "                    markdown_divs = driver.find_elements(By.XPATH, \"//div[contains(@class, 'markdown') or contains(@data-slate-node, 'value')]\")\n",
    "                    if markdown_divs:\n",
    "                        app_proc_container = markdown_divs[0]\n",
    "                    else:\n",
    "                        return {\n",
    "                            \"offline_steps\": \"Section Not Available\", \n",
    "                            \"offline_notes\": \"Section Not Available\",\n",
    "                            \"online_steps\": \"Section Not Available\",\n",
    "                            \"online_notes\": \"Section Not Available\"\n",
    "                        }\n",
    "                except NoSuchElementException:\n",
    "                    return {\n",
    "                        \"offline_steps\": \"Section Not Available\", \n",
    "                        \"offline_notes\": \"Section Not Available\",\n",
    "                        \"online_steps\": \"Section Not Available\",\n",
    "                        \"online_notes\": \"Section Not Available\"\n",
    "                    }\n",
    "        \n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", app_proc_container)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Initialize result dictionary\n",
    "        result = {\n",
    "            \"offline_steps\": \"Section Not Available\",\n",
    "            \"offline_notes\": \"Section Not Available\",\n",
    "            \"online_steps\": \"Section Not Available\",\n",
    "            \"online_notes\": \"Section Not Available\"\n",
    "        }\n",
    "        \n",
    "        # Look for tabs (Offline/Online)\n",
    "        tabs = driver.find_elements(By.XPATH, \"//div[contains(@class, 'px-4') and .//span[contains(text(), 'Offline') or contains(text(), 'Online')]]\")\n",
    "        \n",
    "        # If tabs exist, process each tab separately\n",
    "        if tabs and len(tabs) > 1:\n",
    "            # First, extract content from the currently active tab\n",
    "            active_tab_text = tabs[0].find_element(By.TAG_NAME, \"span\").text.strip().lower()\n",
    "            active_content = extract_content_from_current_tab(driver, app_proc_container)\n",
    "            \n",
    "            if \"offline\" in active_tab_text:\n",
    "                result[\"offline_steps\"] = active_content[\"steps\"]\n",
    "                result[\"offline_notes\"] = active_content[\"notes\"]\n",
    "            elif \"online\" in active_tab_text:\n",
    "                result[\"online_steps\"] = active_content[\"steps\"]\n",
    "                result[\"online_notes\"] = active_content[\"notes\"]\n",
    "            \n",
    "            # Now click on the other tab(s) and extract their content\n",
    "            for tab in tabs[1:]:\n",
    "                try:\n",
    "                    tab_text = tab.find_element(By.TAG_NAME, \"span\").text.strip().lower()\n",
    "                    \n",
    "                    # Skip if we've already processed this tab type\n",
    "                    if (\"offline\" in tab_text and result[\"offline_steps\"] != \"Section Not Available\") or \\\n",
    "                       (\"online\" in tab_text and result[\"online_steps\"] != \"Section Not Available\"):\n",
    "                        continue\n",
    "                    \n",
    "                    # Click the tab\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView();\", tab)\n",
    "                    driver.execute_script(\"arguments[0].click();\", tab)\n",
    "                    time.sleep(2)  # Wait for content to load\n",
    "                    \n",
    "                    # Extract content from the newly active tab\n",
    "                    tab_content = extract_content_from_current_tab(driver, app_proc_container)\n",
    "                    \n",
    "                    # Store content based on tab type\n",
    "                    if \"offline\" in tab_text:\n",
    "                        result[\"offline_steps\"] = tab_content[\"steps\"]\n",
    "                        result[\"offline_notes\"] = tab_content[\"notes\"]\n",
    "                    elif \"online\" in tab_text:\n",
    "                        result[\"online_steps\"] = tab_content[\"steps\"]\n",
    "                        result[\"online_notes\"] = tab_content[\"notes\"]\n",
    "                        \n",
    "                except (ElementClickInterceptedException, NoSuchElementException) as e:\n",
    "                    print(f\"Error clicking or processing tab: {e}\")\n",
    "                    continue\n",
    "        else:\n",
    "            # No tabs found, extract content normally\n",
    "            content = extract_content_from_current_tab(driver, app_proc_container)\n",
    "            \n",
    "            # Determine if content refers to online or offline process\n",
    "            content_text = content[\"steps\"].lower()\n",
    "            if \"online\" in content_text or \"website\" in content_text or \"portal\" in content_text or \"internet\" in content_text:\n",
    "                result[\"online_steps\"] = content[\"steps\"]\n",
    "                result[\"online_notes\"] = content[\"notes\"]\n",
    "            else:\n",
    "                result[\"offline_steps\"] = content[\"steps\"]\n",
    "                result[\"offline_notes\"] = content[\"notes\"]\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting Application Process section from {url}: {e}\")\n",
    "        return {\n",
    "            \"offline_steps\": \"Error extracting section\", \n",
    "            \"offline_notes\": \"Error extracting section\",\n",
    "            \"online_steps\": \"Error extracting section\",\n",
    "            \"online_notes\": \"Error extracting section\"\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process scheme links and extract the Application Process section.\n",
    "    It loads the input file, processes only those schemes that have not yet been processed,\n",
    "    extracts the Application Process section for both online and offline modes,\n",
    "    and saves the data to an output Excel file.\n",
    "    \"\"\"\n",
    "    input_df = load_input()\n",
    "    if input_df is None:\n",
    "        return\n",
    "    print(f\"Loaded {len(input_df)} schemes from {INPUT_FILE}.\")\n",
    "    \n",
    "    checkpoint_df, processed_links = load_checkpoint()\n",
    "    print(f\"Already processed {len(processed_links)} scheme links from checkpoint.\")\n",
    "    \n",
    "    # Create new columns for application process if they don't exist\n",
    "    new_columns = [\n",
    "        \"application_steps_offline\", \"application_notes_offline\", \n",
    "        \"application_steps_online\", \"application_notes_online\"\n",
    "    ]\n",
    "    \n",
    "    for col in new_columns:\n",
    "        if col not in input_df.columns:\n",
    "            input_df[col] = \"\"\n",
    "    \n",
    "    # Filter rows without application process info\n",
    "    no_app_df = input_df[\n",
    "        (input_df[\"application_steps_offline\"].isna() | (input_df[\"application_steps_offline\"] == \"\")) &\n",
    "        (input_df[\"application_steps_online\"].isna() | (input_df[\"application_steps_online\"] == \"\"))\n",
    "    ]\n",
    "    \n",
    "    has_app_df = input_df[\n",
    "        ~((input_df[\"application_steps_offline\"].isna() | (input_df[\"application_steps_offline\"] == \"\")) &\n",
    "          (input_df[\"application_steps_online\"].isna() | (input_df[\"application_steps_online\"] == \"\")))\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(no_app_df)} schemes without Application Process information\")\n",
    "    print(f\"{len(has_app_df)} schemes already have Application Process information\")\n",
    "    \n",
    "    if len(no_app_df) == 0:\n",
    "        print(\"No schemes need processing. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    updated_rows = []\n",
    "    total_to_process = len(no_app_df)\n",
    "    \n",
    "    for idx, row in tqdm(no_app_df.iterrows(), total=total_to_process, desc=\"Processing schemes for Application Process\"):\n",
    "        scheme_name = row[\"scheme_name\"]\n",
    "        scheme_link = row[\"scheme_link\"]\n",
    "        \n",
    "        # Skip if URL is invalid\n",
    "        if not scheme_link or not scheme_link.startswith(\"http\"):\n",
    "            print(f\"Skipping invalid URL for: {scheme_name}\")\n",
    "            updated_row = row.copy()\n",
    "            updated_row[\"application_steps_offline\"] = \"Error: Invalid URL\"\n",
    "            updated_row[\"application_notes_offline\"] = \"Error: Invalid URL\"\n",
    "            updated_row[\"application_steps_online\"] = \"Error: Invalid URL\"\n",
    "            updated_row[\"application_notes_online\"] = \"Error: Invalid URL\"\n",
    "            updated_rows.append(updated_row)\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing scheme: {scheme_name}\")\n",
    "        print(f\"URL: {scheme_link}\")\n",
    "        \n",
    "        app_proc_data = None\n",
    "        retries = 0\n",
    "        backoff_time = 2\n",
    "        \n",
    "        while app_proc_data is None and retries < MAX_RETRIES:\n",
    "            try:\n",
    "                app_proc_data = extract_application_process_section(driver, scheme_link)\n",
    "                \n",
    "                # Check if we got sufficient content from either mode\n",
    "                offline_content_len = len(app_proc_data[\"offline_steps\"]) if app_proc_data[\"offline_steps\"] != \"Section Not Available\" else 0\n",
    "                online_content_len = len(app_proc_data[\"online_steps\"]) if app_proc_data[\"online_steps\"] != \"Section Not Available\" else 0\n",
    "                \n",
    "                if offline_content_len < 10 and online_content_len < 10:\n",
    "                    print(f\"Attempt {retries+1}: Insufficient Application Process content for {scheme_name}\")\n",
    "                    retries += 1\n",
    "                    if retries < MAX_RETRIES:\n",
    "                        print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                        time.sleep(backoff_time)\n",
    "                        backoff_time *= 1.5\n",
    "                        app_proc_data = None\n",
    "                    else:\n",
    "                        app_proc_data = {\n",
    "                            \"offline_steps\": \"Section Not Available against this scheme\", \n",
    "                            \"offline_notes\": \"Section Not Available against this scheme\",\n",
    "                            \"online_steps\": \"Section Not Available against this scheme\",\n",
    "                            \"online_notes\": \"Section Not Available against this scheme\"\n",
    "                        }\n",
    "                else:\n",
    "                    print(f\"Successfully extracted Application Process for {scheme_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {retries+1} failed for {scheme_name}: {e}\")\n",
    "                retries += 1\n",
    "                if retries < MAX_RETRIES:\n",
    "                    print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                    time.sleep(backoff_time)\n",
    "                    backoff_time *= 1.5\n",
    "                else:\n",
    "                    app_proc_data = {\n",
    "                        \"offline_steps\": \"Section Not Available against this scheme\", \n",
    "                        \"offline_notes\": \"Section Not Available against this scheme\",\n",
    "                        \"online_steps\": \"Section Not Available against this scheme\",\n",
    "                        \"online_notes\": \"Section Not Available against this scheme\"\n",
    "                    }\n",
    "        \n",
    "        updated_row = row.copy()\n",
    "        updated_row[\"application_steps_offline\"] = app_proc_data[\"offline_steps\"]\n",
    "        updated_row[\"application_notes_offline\"] = app_proc_data[\"offline_notes\"]\n",
    "        updated_row[\"application_steps_online\"] = app_proc_data[\"online_steps\"]\n",
    "        updated_row[\"application_notes_online\"] = app_proc_data[\"online_notes\"]\n",
    "        updated_rows.append(updated_row)\n",
    "        \n",
    "        if (idx + 1) % 5 == 0 or (idx + 1) == total_to_process:\n",
    "            processed_df = pd.DataFrame(updated_rows)\n",
    "            # Make sure all columns exist in both DataFrames before concatenation\n",
    "            for col in processed_df.columns:\n",
    "                if col not in has_app_df.columns:\n",
    "                    has_app_df[col] = \"\"\n",
    "            for col in has_app_df.columns:\n",
    "                if col not in processed_df.columns:\n",
    "                    processed_df[col] = \"\"\n",
    "                    \n",
    "            temp_df = pd.concat([has_app_df, processed_df], ignore_index=True)\n",
    "            temp_df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "            print(f\"Checkpoint saved. Processed {idx+1}/{total_to_process} schemes without Application Process info.\")\n",
    "        \n",
    "        delay_time = 2 + random.random() * 3\n",
    "        print(f\"Waiting {delay_time:.2f} seconds before next scheme...\")\n",
    "        time.sleep(delay_time)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    # Make sure all columns exist in both DataFrames before concatenation\n",
    "    for col in pd.DataFrame(updated_rows).columns:\n",
    "        if col not in has_app_df.columns:\n",
    "            has_app_df[col] = \"\"\n",
    "            \n",
    "    final_df = pd.concat([has_app_df, pd.DataFrame(updated_rows)], ignore_index=True)\n",
    "    if \"original_index\" in final_df.columns:\n",
    "        final_df = final_df.sort_values(\"original_index\")\n",
    "    final_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nExtraction complete! Updated {len(updated_rows)} schemes.\")\n",
    "    print(f\"Results saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b42851-1e18-47cf-a2d4-3d0e6552c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the Documents required section of each scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcb0673-8e85-4d17-af8a-15af335af0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # for progress reporting\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# File paths\n",
    "INPUT_FILE = \"myscheme_application_process.xlsx\"  # Your input file with scheme links, etc.\n",
    "OUTPUT_FILE = \"myscheme_documents_required.xlsx\"  # Final output file with Documents Required details\n",
    "CHECKPOINT_FILE = \"documents_required_checkpoint.xlsx\"  # Checkpoint file to resume progress\n",
    "\n",
    "# Maximum number of retries for failed extraction attempts\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Configure and return a headless Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def load_input():\n",
    "    \"\"\"\n",
    "    Load the input Excel file with scheme details and ensure an 'original_index' column exists.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Error: {INPUT_FILE} not found. Run the initial scraper first.\")\n",
    "        return None\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    if \"original_index\" not in df.columns:\n",
    "        df[\"original_index\"] = df.index\n",
    "    return df\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"\n",
    "    Load the checkpoint file to track progress.\n",
    "    Returns a DataFrame and a set of processed scheme links.\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        df = pd.read_excel(CHECKPOINT_FILE)\n",
    "        if not df.empty and 'scheme_link' in df.columns:\n",
    "            processed_links = set(df[\"scheme_link\"])\n",
    "            return df, processed_links\n",
    "    # Define required columns for documents required extraction.\n",
    "    columns = [\"scheme_name\", \"scheme_link\", \"page\", \"documents_required\", \"original_index\"]\n",
    "    return pd.DataFrame(columns=columns), set()\n",
    "\n",
    "def save_checkpoint(df):\n",
    "    \"\"\"\n",
    "    Save current progress to the checkpoint file.\n",
    "    \"\"\"\n",
    "    df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "    print(f\"Checkpoint saved with {len(df)} schemes.\")\n",
    "\n",
    "def extract_documents_required(driver, url):\n",
    "    \"\"\"\n",
    "    Extract the 'Documents Required' section content from a scheme page.\n",
    "    Attempts multiple methods:\n",
    "      - First, it looks for a container with id \"documents-required\".\n",
    "      - If not found, it looks for a heading containing \"Documents Required\"\n",
    "        and then takes an ancestor container.\n",
    "      - It then attempts to extract numbered list items, or if that fails, all text.\n",
    "    Returns the formatted text or a default message if the section isn’t found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        time.sleep(3)  # Allow time for JS to render\n",
    "        \n",
    "        # Attempt 1: Look for the container by ID.\n",
    "        try:\n",
    "            docs_container = driver.find_element(By.ID, \"documents-required\")\n",
    "        except NoSuchElementException:\n",
    "            docs_container = None\n",
    "        \n",
    "        # Attempt 2: If not found by ID, try finding a heading with \"Documents Required\"\n",
    "        if not docs_container:\n",
    "            headings = driver.find_elements(\n",
    "                By.XPATH, \n",
    "                \"//h3[contains(translate(text(),'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'documents required')]\"\n",
    "            )\n",
    "            if headings:\n",
    "                # Use an ancestor container; adjust the level as needed.\n",
    "                docs_container = headings[0].find_element(By.XPATH, \"ancestor::div[2]\")\n",
    "            else:\n",
    "                return \"Documents Required section not found on this page\"\n",
    "        \n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", docs_container)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        documents_list = []\n",
    "        \n",
    "        # Method 1: Try to extract numbered items from div elements with data-slate-node=\"element\"\n",
    "        doc_items = docs_container.find_elements(By.XPATH, \".//div[@data-slate-node='element']\")\n",
    "        if doc_items:\n",
    "            for item in doc_items:\n",
    "                text = item.text.strip()\n",
    "                if text and re.match(r'^\\d+\\.', text):\n",
    "                    documents_list.append(text)\n",
    "        \n",
    "        # Method 2: If Method 1 didn't work, look for list items.\n",
    "        if not documents_list:\n",
    "            list_items = docs_container.find_elements(By.XPATH, \".//ol/li | .//ul/li\")\n",
    "            if list_items:\n",
    "                for i, item in enumerate(list_items):\n",
    "                    text = item.text.strip()\n",
    "                    if text:\n",
    "                        documents_list.append(f\"{i+1}. {text}\")\n",
    "        \n",
    "        # Method 3: If still nothing, try to get all paragraph text.\n",
    "        if not documents_list:\n",
    "            paragraphs = docs_container.find_elements(By.TAG_NAME, \"p\")\n",
    "            for p in paragraphs:\n",
    "                text = p.text.strip()\n",
    "                if text:\n",
    "                    documents_list.append(text)\n",
    "        \n",
    "        # Method 4: If nothing so far, look for spans with data-slate-string.\n",
    "        if not documents_list:\n",
    "            spans = docs_container.find_elements(By.XPATH, \".//span[@data-slate-string='true']\")\n",
    "            for span in spans:\n",
    "                text = span.text.strip()\n",
    "                if text:\n",
    "                    documents_list.append(text)\n",
    "        \n",
    "        # Final fallback: get all text from container.\n",
    "        if not documents_list:\n",
    "            all_text = docs_container.text.strip()\n",
    "            if all_text:\n",
    "                # Optionally, split by newline and remove the heading if present.\n",
    "                lines = re.split(r'\\n+', all_text)\n",
    "                for line in lines:\n",
    "                    cleaned = line.strip()\n",
    "                    if cleaned and not cleaned.lower().startswith(\"documents required\"):\n",
    "                        documents_list.append(cleaned)\n",
    "        \n",
    "        if documents_list:\n",
    "            return \"\\n\".join(documents_list)\n",
    "        else:\n",
    "            return \"No document requirements found in this section\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting Documents Required section from {url}: {e}\")\n",
    "        return \"Error extracting Documents Required section\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process scheme links and extract the Documents Required section.\n",
    "    It loads the input file, processes only those schemes that have not yet been processed,\n",
    "    extracts the Documents Required section from each scheme, and saves the data to an output Excel file.\n",
    "    \"\"\"\n",
    "    input_df = load_input()\n",
    "    if input_df is None:\n",
    "        return\n",
    "    print(f\"Loaded {len(input_df)} schemes from {INPUT_FILE}.\")\n",
    "    \n",
    "    checkpoint_df, processed_links = load_checkpoint()\n",
    "    print(f\"Already processed {len(processed_links)} scheme links from checkpoint.\")\n",
    "    \n",
    "    # Create a new column for documents_required if it doesn't exist.\n",
    "    if \"documents_required\" not in input_df.columns:\n",
    "        input_df[\"documents_required\"] = \"\"\n",
    "    \n",
    "    # Filter rows without documents_required info.\n",
    "    no_docs_df = input_df[input_df[\"documents_required\"].isna() | (input_df[\"documents_required\"] == \"\")]\n",
    "    has_docs_df = input_df[~(input_df[\"documents_required\"].isna() | (input_df[\"documents_required\"] == \"\"))]\n",
    "    \n",
    "    print(f\"Found {len(no_docs_df)} schemes without Documents Required information\")\n",
    "    print(f\"{len(has_docs_df)} schemes already have Documents Required information\")\n",
    "    \n",
    "    if len(no_docs_df) == 0:\n",
    "        print(\"No schemes need processing. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    driver = setup_driver()\n",
    "    updated_rows = []\n",
    "    total_to_process = len(no_docs_df)\n",
    "    \n",
    "    for idx, row in tqdm(no_docs_df.iterrows(), total=total_to_process, desc=\"Processing schemes for Documents Required\"):\n",
    "        scheme_name = row[\"scheme_name\"]\n",
    "        scheme_link = row[\"scheme_link\"]\n",
    "        \n",
    "        # Skip invalid URLs.\n",
    "        if not scheme_link or not scheme_link.startswith(\"http\"):\n",
    "            print(f\"Skipping invalid URL for: {scheme_name}\")\n",
    "            updated_row = row.copy()\n",
    "            updated_row[\"documents_required\"] = \"Error: Invalid URL\"\n",
    "            updated_rows.append(updated_row)\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing scheme: {scheme_name}\")\n",
    "        print(f\"URL: {scheme_link}\")\n",
    "        \n",
    "        docs_text = None\n",
    "        retries = 0\n",
    "        backoff_time = 2\n",
    "        \n",
    "        while docs_text is None and retries < MAX_RETRIES:\n",
    "            try:\n",
    "                docs_text = extract_documents_required(driver, scheme_link)\n",
    "                if not docs_text or len(docs_text) < 10:\n",
    "                    print(f\"Attempt {retries+1}: Insufficient Documents Required content for {scheme_name}\")\n",
    "                    retries += 1\n",
    "                    if retries < MAX_RETRIES:\n",
    "                        print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                        time.sleep(backoff_time)\n",
    "                        backoff_time *= 1.5\n",
    "                        docs_text = None\n",
    "                    else:\n",
    "                        docs_text = \"Section Not Available against this scheme\"\n",
    "                else:\n",
    "                    print(f\"Successfully extracted Documents Required for {scheme_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {retries+1} failed for {scheme_name}: {e}\")\n",
    "                retries += 1\n",
    "                if retries < MAX_RETRIES:\n",
    "                    print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                    time.sleep(backoff_time)\n",
    "                    backoff_time *= 1.5\n",
    "                else:\n",
    "                    docs_text = \"Section Not Available against this scheme\"\n",
    "        \n",
    "        updated_row = row.copy()\n",
    "        updated_row[\"documents_required\"] = docs_text\n",
    "        updated_rows.append(updated_row)\n",
    "        \n",
    "        # Save checkpoint every 5 schemes or at the end.\n",
    "        if (idx + 1) % 5 == 0 or (idx + 1) == total_to_process:\n",
    "            processed_df = pd.DataFrame(updated_rows)\n",
    "            temp_df = pd.concat([has_docs_df, processed_df], ignore_index=True)\n",
    "            temp_df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "            print(f\"Checkpoint saved. Processed {idx+1}/{total_to_process} schemes.\")\n",
    "        \n",
    "        delay_time = 2 + random.random() * 3\n",
    "        print(f\"Waiting {delay_time:.2f} seconds before next scheme...\")\n",
    "        time.sleep(delay_time)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    final_df = pd.concat([has_docs_df, pd.DataFrame(updated_rows)], ignore_index=True)\n",
    "    if \"original_index\" in final_df.columns:\n",
    "        final_df = final_df.sort_values(\"original_index\")\n",
    "    final_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nExtraction complete! Updated {len(updated_rows)} new schemes.\")\n",
    "    print(f\"Results saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ada1d5-70a7-443f-8444-994823db45ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the Scheme type section of each scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160fbab-6678-4e1e-93cf-c99c9ab788ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm import tqdm  # for progress reporting\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# This will download (if needed) and return the path of the latest chromedriver\n",
    "driver_path = ChromeDriverManager().install()\n",
    "service = Service(driver_path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Now you can use your driver as usual\n",
    "driver.get(\"https://www.google.com\")\n",
    "print(\"Latest chromedriver version is being used.\")\n",
    "driver.quit()\n",
    "\n",
    "# File paths (adjust as needed)\n",
    "INPUT_FILE = \"myscheme_documents_required.xlsx\"  # Your input file with scheme_name, scheme_link, etc.\n",
    "OUTPUT_FILE = \"myscheme_scheme_type.xlsx\"         # Final output file with Scheme Type information\n",
    "CHECKPOINT_FILE = \"scheme_type_checkpoint.xlsx\"   # Checkpoint file to resume progress\n",
    "\n",
    "# Maximum number of retries for failed extraction attempts\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# List of Indian state names (expand as needed)\n",
    "STATE_NAMES = [\n",
    "    \"Andhra Pradesh\", \"Arunachal Pradesh\", \"Assam\", \"Bihar\", \"Chhattisgarh\", \"Goa\", \n",
    "    \"Gujarat\", \"Haryana\", \"Himachal Pradesh\", \"Jharkhand\", \"Karnataka\", \"Kerala\", \n",
    "    \"Madhya Pradesh\", \"Maharashtra\", \"Manipur\", \"Meghalaya\", \"Mizoram\", \"Nagaland\", \n",
    "    \"Odisha\", \"Punjab\", \"Rajasthan\", \"Sikkim\", \"Tamil Nadu\", \"Telangana\", \"Tripura\", \n",
    "    \"Uttar Pradesh\", \"Uttarakhand\", \"West Bengal\", \"Delhi\", \"Puducherry\", \"Chandigarh\", \n",
    "    \"Andaman and Nicobar Islands\", \"Dadra and Nagar Haveli and Daman and Diu\"\n",
    "]\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "                    handlers=[logging.StreamHandler()])\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Configure and return a headless Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def load_input():\n",
    "    \"\"\"\n",
    "    Load the input Excel file with scheme details and ensure an 'original_index' column exists.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        logging.error(f\"Error: {INPUT_FILE} not found. Run the initial scraper first.\")\n",
    "        return None\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    if \"original_index\" not in df.columns:\n",
    "        df[\"original_index\"] = df.index\n",
    "    return df\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"\n",
    "    Load the checkpoint file to track progress.\n",
    "    Returns a DataFrame and a set of processed scheme links.\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        df = pd.read_excel(CHECKPOINT_FILE)\n",
    "        if not df.empty and 'scheme_link' in df.columns:\n",
    "            processed_links = set(df[\"scheme_link\"])\n",
    "            return df, processed_links\n",
    "    columns = [\"scheme_name\", \"scheme_link\", \"page\", \"scheme_type\", \"original_index\"]\n",
    "    return pd.DataFrame(columns=columns), set()\n",
    "\n",
    "def save_checkpoint(df):\n",
    "    \"\"\"\n",
    "    Save current progress to the checkpoint file.\n",
    "    \"\"\"\n",
    "    df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "    logging.info(f\"Checkpoint saved with {len(df)} schemes.\")\n",
    "\n",
    "def extract_scheme_type(driver, url):\n",
    "    \"\"\"\n",
    "    Extract the scheme type from a scheme page.\n",
    "    \n",
    "    This function loads the page, scrolls to a likely area where the scheme type is displayed,\n",
    "    and then searches for known state names. If any state name is found (case-insensitive),\n",
    "    it returns \"State Scheme\"; otherwise, it returns \"Central Scheme\".\n",
    "    \n",
    "    If no information is found, it returns \"Scheme Type Not Available\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(3)  # Allow JS to render\n",
    "        \n",
    "        # Strategy 1: Try to find an <h3> element that might display the scheme type.\n",
    "        try:\n",
    "            type_elem = driver.find_element(By.XPATH, \"//h3[contains(@class, 'text-') and contains(@class, 'cursor-pointer')]\")\n",
    "            type_text = type_elem.text.strip()\n",
    "            logging.debug(f\"Found scheme type text via h3: {type_text}\")\n",
    "        except NoSuchElementException:\n",
    "            type_text = \"\"\n",
    "        \n",
    "        # Strategy 2: If not found, try to find any element that contains \"Ministry\"\n",
    "        if not type_text:\n",
    "            try:\n",
    "                type_elem = driver.find_element(By.XPATH, \"//div[contains(text(),'Ministry')]\")\n",
    "                type_text = type_elem.text.strip()\n",
    "                logging.debug(f\"Found scheme type text via div: {type_text}\")\n",
    "            except NoSuchElementException:\n",
    "                type_text = \"\"\n",
    "        \n",
    "        # Strategy 3: If still not found, get body text and search for state names\n",
    "        if not type_text:\n",
    "            body_text = driver.find_element(By.TAG_NAME, \"body\").text.strip()\n",
    "            type_text = body_text[:200]  # take first 200 chars as a guess\n",
    "        \n",
    "        if type_text:\n",
    "            # Check if any state name is in the text\n",
    "            for state in STATE_NAMES:\n",
    "                if state.lower() in type_text.lower():\n",
    "                    logging.info(f\"Scheme type determined as State Scheme based on state name: {state}\")\n",
    "                    return \"State Scheme\"\n",
    "            # If no state name found, assume central scheme.\n",
    "            logging.info(\"Scheme type determined as Central Scheme\")\n",
    "            return \"Central Scheme\"\n",
    "        else:\n",
    "            return \"Scheme Type Not Available\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting scheme type from {url}: {e}\")\n",
    "        return \"Error extracting scheme type\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process scheme links and extract the Scheme Type.\n",
    "    Loads the input file, skips already processed schemes (via checkpoint),\n",
    "    extracts the Scheme Type for each scheme, and saves the results to an output Excel file.\n",
    "    \"\"\"\n",
    "    input_df = load_input()\n",
    "    if input_df is None:\n",
    "        return\n",
    "    logging.info(f\"Loaded {len(input_df)} schemes from {INPUT_FILE}.\")\n",
    "    \n",
    "    checkpoint_df, processed_links = load_checkpoint()\n",
    "    logging.info(f\"Already processed {len(processed_links)} scheme links from checkpoint.\")\n",
    "    \n",
    "    # Process only unprocessed schemes based on unique scheme_link\n",
    "    unprocessed_df = input_df[~input_df[\"scheme_link\"].isin(processed_links)]\n",
    "    logging.info(f\"Processing {len(unprocessed_df)} new schemes for Scheme Type extraction.\")\n",
    "    \n",
    "    updated_rows = []\n",
    "    total_to_process = len(unprocessed_df)\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    for idx, row in tqdm(unprocessed_df.iterrows(), total=total_to_process, desc=\"Processing Scheme Type\"):\n",
    "        scheme_name = row[\"scheme_name\"]\n",
    "        scheme_link = row[\"scheme_link\"]\n",
    "        \n",
    "        if not scheme_link or not scheme_link.startswith(\"http\"):\n",
    "            logging.warning(f\"Skipping invalid URL for: {scheme_name}\")\n",
    "            updated_row = row.copy()\n",
    "            updated_row[\"scheme_type\"] = \"Error: Invalid URL\"\n",
    "            updated_rows.append(updated_row)\n",
    "            continue\n",
    "        \n",
    "        logging.info(f\"\\nProcessing scheme: {scheme_name}\")\n",
    "        logging.info(f\"URL: {scheme_link}\")\n",
    "        \n",
    "        scheme_type = None\n",
    "        retries = 0\n",
    "        backoff_time = 2\n",
    "        \n",
    "        while scheme_type is None and retries < MAX_RETRIES:\n",
    "            try:\n",
    "                scheme_type = extract_scheme_type(driver, scheme_link)\n",
    "                if not scheme_type or len(scheme_type) < 3 or \"Not Available\" in scheme_type:\n",
    "                    logging.warning(f\"Attempt {retries+1}: Insufficient scheme type info for {scheme_name}\")\n",
    "                    retries += 1\n",
    "                    if retries < MAX_RETRIES:\n",
    "                        logging.info(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                        time.sleep(backoff_time)\n",
    "                        backoff_time *= 1.5\n",
    "                        scheme_type = None\n",
    "                    else:\n",
    "                        scheme_type = \"Section Not Available against this scheme\"\n",
    "                else:\n",
    "                    logging.info(f\"Successfully extracted scheme type: {scheme_type}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Attempt {retries+1} failed for {scheme_name}: {e}\")\n",
    "                retries += 1\n",
    "                if retries < MAX_RETRIES:\n",
    "                    logging.info(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                    time.sleep(backoff_time)\n",
    "                    backoff_time *= 1.5\n",
    "                else:\n",
    "                    scheme_type = \"Section Not Available against this scheme\"\n",
    "        \n",
    "        updated_row = row.copy()\n",
    "        updated_row[\"scheme_type\"] = scheme_type\n",
    "        updated_rows.append(updated_row)\n",
    "        \n",
    "        if (idx + 1) % 5 == 0 or (idx + 1) == total_to_process:\n",
    "            processed_df = pd.DataFrame(updated_rows)\n",
    "            temp_df = pd.concat([checkpoint_df, processed_df], ignore_index=True)\n",
    "            save_checkpoint(temp_df)\n",
    "        \n",
    "        delay_time = 2 + random.random() * 3\n",
    "        logging.info(f\"Waiting {delay_time:.2f} seconds before next scheme...\")\n",
    "        time.sleep(delay_time)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    final_df = pd.concat([checkpoint_df, pd.DataFrame(updated_rows)], ignore_index=True)\n",
    "    if \"original_index\" in final_df.columns:\n",
    "        final_df = final_df.sort_values(\"original_index\")\n",
    "    final_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    logging.info(f\"\\nExtraction complete! Updated {len(updated_rows)} new schemes.\")\n",
    "    logging.info(f\"Results saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963fc67c-d9fd-4985-8033-2f17b68837f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the FAQ section of each scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd4021-cc59-4216-8a21-46a7b96d0653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # for progress reporting\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# File paths (adjust as needed)\n",
    "INPUT_FILE = \"myscheme_scheme_type.xlsx\"  # Your input file with scheme_name, scheme_link, etc.\n",
    "OUTPUT_FILE = \"myscheme_faqs.xlsx\"              # Final output file with FAQ details\n",
    "CHECKPOINT_FILE = \"faqs_checkpoint.xlsx\"        # Checkpoint file to resume progress\n",
    "\n",
    "# Maximum number of retries for failed extraction attempts\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Configure and return a headless Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def load_input():\n",
    "    \"\"\"\n",
    "    Load the input Excel file with scheme details and ensure an 'original_index' column exists.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Error: {INPUT_FILE} not found. Run the initial scraper first.\")\n",
    "        return None\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    if \"original_index\" not in df.columns:\n",
    "        df[\"original_index\"] = df.index\n",
    "    return df\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"\n",
    "    Load the checkpoint file to track progress.\n",
    "    Returns a DataFrame and a set of processed scheme links.\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        df = pd.read_excel(CHECKPOINT_FILE)\n",
    "        if not df.empty and 'scheme_link' in df.columns:\n",
    "            processed_links = set(df[\"scheme_link\"])\n",
    "            return df, processed_links\n",
    "    columns = [\"scheme_name\", \"scheme_link\", \"page\", \"faqs\", \"original_index\"]\n",
    "    return pd.DataFrame(columns=columns), set()\n",
    "\n",
    "def save_checkpoint(df):\n",
    "    \"\"\"\n",
    "    Save current progress to the checkpoint file.\n",
    "    \"\"\"\n",
    "    df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "    print(f\"Checkpoint saved with {len(df)} schemes.\")\n",
    "\n",
    "def extract_faqs_section(driver, url):\n",
    "    \"\"\"\n",
    "    Extract the Frequently Asked Questions (FAQs) section from a scheme page.\n",
    "    \n",
    "    This function loads the given URL, scrolls to the FAQ container (with id=\"faqs\"),\n",
    "    and then iterates over FAQ items. For each FAQ, it:\n",
    "      - Extracts the question text (from a <p> element that appears to contain the question)\n",
    "      - Checks if the answer container (a sibling <div> with a class containing \"rounded-b\" or similar)\n",
    "        is hidden; if it is, it simulates a click on the question to reveal the answer.\n",
    "      - Extracts the answer text.\n",
    "    \n",
    "    The function returns a single string that contains a numbered list of FAQs in the format:\n",
    "      1. Q: <question>\n",
    "         A: <answer>\n",
    "      2. Q: <question>\n",
    "         A: <answer>\n",
    "    If the section is not found, returns \"FAQs section not available\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(3)  # allow extra time for JS to render\n",
    "        \n",
    "        try:\n",
    "            faq_container = driver.find_element(By.ID, \"faqs\")\n",
    "        except NoSuchElementException:\n",
    "            return \"FAQs section not available\"\n",
    "        \n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", faq_container)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Assume FAQ items are within a grid container inside #faqs.\n",
    "        # Adjust the XPath if needed.\n",
    "        faq_items = faq_container.find_elements(By.XPATH, \".//div[contains(@class, 'py-4')]\")\n",
    "        \n",
    "        if not faq_items:\n",
    "            return \"No FAQs found\"\n",
    "        \n",
    "        faq_list = []\n",
    "        question_counter = 1\n",
    "        \n",
    "        for item in faq_items:\n",
    "            try:\n",
    "                # Look for a question element (e.g., a <p> with bold text)\n",
    "                question_elem = item.find_element(By.XPATH, \".//p[contains(@class, 'font-bold')]\")\n",
    "                question_text = question_elem.text.strip()\n",
    "            except NoSuchElementException:\n",
    "                question_text = \"\"\n",
    "            \n",
    "            # Look for the answer container.\n",
    "            # In your HTML sample, the answer appears in a sibling div (with classes like \"rounded-b\" etc.)\n",
    "            try:\n",
    "                answer_elem = item.find_element(By.XPATH, \".//div[contains(@class, 'rounded-b')]\")\n",
    "                # If the answer element has class \"hidden\", click the question to reveal it.\n",
    "                if \"hidden\" in answer_elem.get_attribute(\"class\").lower():\n",
    "                    try:\n",
    "                        question_elem.click()\n",
    "                        time.sleep(1)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not click question to reveal answer: {e}\")\n",
    "                answer_text = answer_elem.text.strip()\n",
    "            except NoSuchElementException:\n",
    "                answer_text = \"\"\n",
    "            \n",
    "            # If answer text is empty, try to fetch any text from the item.\n",
    "            if not answer_text:\n",
    "                answer_text = item.text.strip()\n",
    "            \n",
    "            # If we have a valid question and answer, add to the FAQ list.\n",
    "            if question_text:\n",
    "                faq_list.append(f\"{question_counter}. Q: {question_text}\\n   A: {answer_text}\")\n",
    "                question_counter += 1\n",
    "        \n",
    "        if faq_list:\n",
    "            return \"\\n\\n\".join(faq_list)\n",
    "        else:\n",
    "            return \"No FAQs found\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting FAQs from {url}: {e}\")\n",
    "        return \"Error extracting FAQs\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process scheme links and extract the FAQs section.\n",
    "    It loads the input file, processes only those schemes that haven't been processed yet,\n",
    "    extracts the FAQs from each scheme, and saves the results to an output Excel file.\n",
    "    \"\"\"\n",
    "    input_df = load_input()\n",
    "    if input_df is None:\n",
    "        return\n",
    "    print(f\"Loaded {len(input_df)} schemes from {INPUT_FILE}.\")\n",
    "    \n",
    "    checkpoint_df, processed_links = load_checkpoint()\n",
    "    print(f\"Already processed {len(processed_links)} scheme links from checkpoint.\")\n",
    "    \n",
    "    # Process only schemes not in the checkpoint\n",
    "    unprocessed_df = input_df[~input_df[\"scheme_link\"].isin(processed_links)]\n",
    "    print(f\"Processing {len(unprocessed_df)} new schemes for FAQs extraction.\")\n",
    "    \n",
    "    # Create a new column for FAQs if it doesn't exist.\n",
    "    if \"faqs\" not in input_df.columns:\n",
    "        input_df[\"faqs\"] = \"\"\n",
    "    \n",
    "    updated_rows = []\n",
    "    total_to_process = len(unprocessed_df)\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    for idx, row in tqdm(unprocessed_df.iterrows(), total=total_to_process, desc=\"Processing FAQs\"):\n",
    "        scheme_name = row[\"scheme_name\"]\n",
    "        scheme_link = row[\"scheme_link\"]\n",
    "        \n",
    "        if not scheme_link or not scheme_link.startswith(\"http\"):\n",
    "            print(f\"Skipping invalid URL for: {scheme_name}\")\n",
    "            updated_row = row.copy()\n",
    "            updated_row[\"faqs\"] = \"Error: Invalid URL\"\n",
    "            updated_rows.append(updated_row)\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing scheme: {scheme_name}\")\n",
    "        print(f\"URL: {scheme_link}\")\n",
    "        \n",
    "        faqs_text = None\n",
    "        retries = 0\n",
    "        backoff_time = 2\n",
    "        \n",
    "        while faqs_text is None and retries < MAX_RETRIES:\n",
    "            try:\n",
    "                faqs_text = extract_faqs_section(driver, scheme_link)\n",
    "                if not faqs_text or len(faqs_text) < 10:\n",
    "                    print(f\"Attempt {retries+1}: Insufficient FAQs content for {scheme_name}\")\n",
    "                    retries += 1\n",
    "                    if retries < MAX_RETRIES:\n",
    "                        print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                        time.sleep(backoff_time)\n",
    "                        backoff_time *= 1.5\n",
    "                        faqs_text = None\n",
    "                    else:\n",
    "                        faqs_text = \"Section Not Available against this scheme\"\n",
    "                else:\n",
    "                    print(f\"Successfully extracted FAQs for {scheme_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {retries+1} failed for {scheme_name}: {e}\")\n",
    "                retries += 1\n",
    "                if retries < MAX_RETRIES:\n",
    "                    print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                    time.sleep(backoff_time)\n",
    "                    backoff_time *= 1.5\n",
    "                else:\n",
    "                    faqs_text = \"Section Not Available against this scheme\"\n",
    "        \n",
    "        updated_row = row.copy()\n",
    "        updated_row[\"faqs\"] = faqs_text\n",
    "        updated_rows.append(updated_row)\n",
    "        \n",
    "        if (idx + 1) % 5 == 0 or (idx + 1) == total_to_process:\n",
    "            processed_df = pd.DataFrame(updated_rows)\n",
    "            temp_df = pd.concat([checkpoint_df, processed_df], ignore_index=True)\n",
    "            temp_df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "            print(f\"Checkpoint saved. Processed {idx+1}/{total_to_process} schemes.\")\n",
    "        \n",
    "        delay_time = 2 + random.random() * 3\n",
    "        print(f\"Waiting {delay_time:.2f} seconds before next scheme...\")\n",
    "        time.sleep(delay_time)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    final_df = pd.concat([checkpoint_df, pd.DataFrame(updated_rows)], ignore_index=True)\n",
    "    if \"original_index\" in final_df.columns:\n",
    "        final_df = final_df.sort_values(\"original_index\")\n",
    "    final_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nExtraction complete! Updated {len(updated_rows)} new schemes.\")\n",
    "    print(f\"Results saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b123558-3db9-4728-bcbf-dbe9f72c6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the sources section of each scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130732be-9be5-40fc-99bc-b75ec970e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress reporting\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# File paths\n",
    "INPUT_FILE = \"myscheme_faqs.xlsx\"  # Your input file with scheme_name, scheme_link, etc.\n",
    "OUTPUT_FILE = \"myscheme_scheme_sources.xlsx\"    # Final output file with Sources And References details\n",
    "CHECKPOINT_FILE = \"scheme_sources_checkpoint.xlsx\"  # Checkpoint file to resume progress\n",
    "\n",
    "# Maximum number of retries for failed extraction attempts\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Configure and return a headless Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def load_input():\n",
    "    \"\"\"\n",
    "    Load the input Excel file with scheme details and ensure an 'original_index' column exists.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Error: {INPUT_FILE} not found. Run the initial scraper first.\")\n",
    "        return None\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    if \"original_index\" not in df.columns:\n",
    "        df[\"original_index\"] = df.index\n",
    "    return df\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"\n",
    "    Load the checkpoint file to track progress.\n",
    "    Returns a DataFrame and a set of processed scheme links.\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        df = pd.read_excel(CHECKPOINT_FILE)\n",
    "        if not df.empty and 'scheme_link' in df.columns:\n",
    "            processed_links = set(df[\"scheme_link\"])\n",
    "            return df, processed_links\n",
    "    columns = [\"scheme_name\", \"scheme_link\", \"page\", \"sources\", \"original_index\"]\n",
    "    return pd.DataFrame(columns=columns), set()\n",
    "\n",
    "def save_checkpoint(df):\n",
    "    \"\"\"\n",
    "    Save current progress to the checkpoint file.\n",
    "    \"\"\"\n",
    "    df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "    print(f\"Checkpoint saved with {len(df)} schemes.\")\n",
    "\n",
    "def extract_sources_section(driver, url):\n",
    "    \"\"\"\n",
    "    Extract the 'Sources And References' section from a scheme page.\n",
    "    \n",
    "    This function loads the given URL, scrolls to the section (identified by id=\"sources\" or\n",
    "    by a heading containing \"Sources And References\"), and then attempts to extract the sources.\n",
    "    \n",
    "    It searches within the container for anchor (<a>) tags (which are typically links) and/or paragraph (<p>) tags.\n",
    "    For each source found, it numbers the source and, if an anchor tag is present, appends the URL in parentheses.\n",
    "    \n",
    "    If no source information is found, it returns a default message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(3)  # Allow extra time for JS to render\n",
    "        \n",
    "        # Attempt 1: Locate the Sources container by ID\n",
    "        try:\n",
    "            sources_container = driver.find_element(By.ID, \"sources\")\n",
    "        except NoSuchElementException:\n",
    "            sources_container = None\n",
    "        \n",
    "        # Attempt 2: If not found by ID, try finding a heading with \"Sources And References\"\n",
    "        if not sources_container:\n",
    "            headings = driver.find_elements(By.XPATH,\n",
    "                \"//h3[contains(translate(text(),'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'sources and references')]\"\n",
    "            )\n",
    "            if headings:\n",
    "                sources_container = headings[0].find_element(By.XPATH, \"ancestor::div[1]\")\n",
    "            else:\n",
    "                return \"Sources And References section not available\"\n",
    "        \n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", sources_container)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        sources_list = []\n",
    "        \n",
    "        # Method 1: Look for anchor tags within the container.\n",
    "        anchors = sources_container.find_elements(By.TAG_NAME, \"a\")\n",
    "        if anchors:\n",
    "            for i, a in enumerate(anchors):\n",
    "                link_text = a.text.strip()\n",
    "                href = a.get_attribute(\"href\")\n",
    "                # Sometimes the anchor might be empty, so try to get the parent text if available.\n",
    "                if not link_text:\n",
    "                    link_text = a.find_element(By.XPATH, \"./..\").text.strip()\n",
    "                if link_text or href:\n",
    "                    sources_list.append(f\"{i+1}. {link_text} ({href})\")\n",
    "        \n",
    "        # Method 2: If no anchors or if the list is empty, look for paragraph elements.\n",
    "        if not sources_list:\n",
    "            paragraphs = sources_container.find_elements(By.TAG_NAME, \"p\")\n",
    "            if paragraphs:\n",
    "                for i, p in enumerate(paragraphs):\n",
    "                    text = p.text.strip()\n",
    "                    if text:\n",
    "                        sources_list.append(f\"{i+1}. {text}\")\n",
    "        \n",
    "        # Method 3: Fallback - get all text and try to split by newlines\n",
    "        if not sources_list:\n",
    "            all_text = sources_container.text.strip()\n",
    "            if all_text:\n",
    "                lines = [line.strip() for line in all_text.split(\"\\n\") if line.strip()]\n",
    "                for i, line in enumerate(lines):\n",
    "                    # Skip lines that are part of the heading if needed.\n",
    "                    sources_list.append(f\"{i+1}. {line}\")\n",
    "        \n",
    "        if sources_list:\n",
    "            return \"\\n\".join(sources_list)\n",
    "        else:\n",
    "            return \"No Sources And References information found\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting Sources And References section from {url}: {e}\")\n",
    "        return \"Error extracting Sources And References section\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process scheme links and extract the Sources And References section.\n",
    "    It loads the input file, processes only schemes not already processed (using checkpoint data),\n",
    "    extracts the Sources section for each scheme, and saves the results to an output Excel file.\n",
    "    \"\"\"\n",
    "    input_df = load_input()\n",
    "    if input_df is None:\n",
    "        return\n",
    "    print(f\"Loaded {len(input_df)} schemes from {INPUT_FILE}.\")\n",
    "    \n",
    "    checkpoint_df, processed_links = load_checkpoint()\n",
    "    print(f\"Already processed {len(processed_links)} scheme links from checkpoint.\")\n",
    "    \n",
    "    # Process only unprocessed schemes based on scheme_link uniqueness\n",
    "    unprocessed_df = input_df[~input_df[\"scheme_link\"].isin(processed_links)]\n",
    "    print(f\"Processing {len(unprocessed_df)} new schemes for Sources And References extraction.\")\n",
    "    \n",
    "    # Create a new column for sources if it doesn't exist.\n",
    "    if \"sources\" not in input_df.columns:\n",
    "        input_df[\"sources\"] = \"\"\n",
    "    \n",
    "    updated_rows = []\n",
    "    total_to_process = len(unprocessed_df)\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    for idx, row in tqdm(unprocessed_df.iterrows(), total=total_to_process, desc=\"Processing Sources\"):\n",
    "        scheme_name = row[\"scheme_name\"]\n",
    "        scheme_link = row[\"scheme_link\"]\n",
    "        \n",
    "        if not scheme_link or not scheme_link.startswith(\"http\"):\n",
    "            print(f\"Skipping invalid URL for: {scheme_name}\")\n",
    "            updated_row = row.copy()\n",
    "            updated_row[\"sources\"] = \"Error: Invalid URL\"\n",
    "            updated_rows.append(updated_row)\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing scheme: {scheme_name}\")\n",
    "        print(f\"URL: {scheme_link}\")\n",
    "        \n",
    "        sources_text = None\n",
    "        retries = 0\n",
    "        backoff_time = 2\n",
    "        \n",
    "        while sources_text is None and retries < MAX_RETRIES:\n",
    "            try:\n",
    "                sources_text = extract_sources_section(driver, scheme_link)\n",
    "                if not sources_text or len(sources_text) < 10:\n",
    "                    print(f\"Attempt {retries+1}: Insufficient Sources content for {scheme_name}\")\n",
    "                    retries += 1\n",
    "                    if retries < MAX_RETRIES:\n",
    "                        print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                        time.sleep(backoff_time)\n",
    "                        backoff_time *= 1.5\n",
    "                        sources_text = None\n",
    "                    else:\n",
    "                        sources_text = \"Section Not Available against this scheme\"\n",
    "                else:\n",
    "                    print(f\"Successfully extracted Sources And References for {scheme_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {retries+1} failed for {scheme_name}: {e}\")\n",
    "                retries += 1\n",
    "                if retries < MAX_RETRIES:\n",
    "                    print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                    time.sleep(backoff_time)\n",
    "                    backoff_time *= 1.5\n",
    "                else:\n",
    "                    sources_text = \"Section Not Available against this scheme\"\n",
    "        \n",
    "        updated_row = row.copy()\n",
    "        updated_row[\"sources\"] = sources_text\n",
    "        updated_rows.append(updated_row)\n",
    "        \n",
    "        if (idx + 1) % 5 == 0 or (idx + 1) == total_to_process:\n",
    "            processed_df = pd.DataFrame(updated_rows)\n",
    "            temp_df = pd.concat([checkpoint_df, processed_df], ignore_index=True)\n",
    "            save_checkpoint(temp_df)\n",
    "            print(f\"Checkpoint saved. Processed {idx+1}/{total_to_process} schemes.\")\n",
    "        \n",
    "        delay_time = 2 + random.random() * 3\n",
    "        print(f\"Waiting {delay_time:.2f} seconds before next scheme...\")\n",
    "        time.sleep(delay_time)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    final_df = pd.concat([checkpoint_df, pd.DataFrame(updated_rows)], ignore_index=True)\n",
    "    if \"original_index\" in final_df.columns:\n",
    "        final_df = final_df.sort_values(\"original_index\")\n",
    "    final_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nExtraction complete! Updated {len(updated_rows)} new schemes.\")\n",
    "    print(f\"Results saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617e074-6b48-4f88-90d3-911b410a3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the Eligibility Criteria section of each scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccb5c4-c894-4c0d-911b-497a05a324f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # for progress reporting\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# File paths\n",
    "INPUT_FILE = \"myscheme_scheme_sources.xlsx\"       # Your input file with scheme details (scheme_name, scheme_link, etc.)\n",
    "OUTPUT_FILE = \"myscheme_full_details.xlsx\"    # Final output file with Eligibility Criteria data\n",
    "CHECKPOINT_FILE = \"eligibility_checkpoint.xlsx\"       # Checkpoint file to resume progress\n",
    "\n",
    "# Maximum number of retries for failed extraction attempts\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"\n",
    "    Configure and return a headless Chrome WebDriver.\n",
    "    \"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    )\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def load_input():\n",
    "    \"\"\"\n",
    "    Load the input Excel file with scheme details and ensure an 'original_index' column exists.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Error: {INPUT_FILE} not found. Run the initial scraper first.\")\n",
    "        return None\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    if \"original_index\" not in df.columns:\n",
    "        df[\"original_index\"] = df.index\n",
    "    return df\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"\n",
    "    Load the checkpoint file to track progress.\n",
    "    Returns a DataFrame and a set of processed scheme links.\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        df = pd.read_excel(CHECKPOINT_FILE)\n",
    "        if not df.empty and 'scheme_link' in df.columns:\n",
    "            processed_links = set(df[\"scheme_link\"])\n",
    "            return df, processed_links\n",
    "    columns = [\"scheme_name\", \"scheme_link\", \"page\", \"eligibility_criteria\", \"original_index\"]\n",
    "    return pd.DataFrame(columns=columns), set()\n",
    "\n",
    "def save_checkpoint(df):\n",
    "    \"\"\"\n",
    "    Save current progress to the checkpoint file.\n",
    "    \"\"\"\n",
    "    df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "    print(f\"Checkpoint saved with {len(df)} schemes.\")\n",
    "\n",
    "def extract_eligibility_criteria(driver, url):\n",
    "    \"\"\"\n",
    "    Extract the eligibility criteria from a scheme page.\n",
    "    Ensures no duplicate criteria are returned.\n",
    "    \n",
    "    Returns a string of eligibility criteria or \"No eligibility criteria found\" if none exists.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        time.sleep(3)  # Allow JS to render\n",
    "        \n",
    "        # Initialize an empty set to store unique criteria\n",
    "        criteria_set = set()\n",
    "        \n",
    "        # Try multiple approaches to find eligibility criteria\n",
    "        \n",
    "        # Approach 1: Find the main container first\n",
    "        try:\n",
    "            containers = driver.find_elements(By.XPATH, \n",
    "                \"//div[contains(@class, 'grid grid-cols-1 md:flex flex-wrap gap-4')]\")\n",
    "            \n",
    "            for container in containers:\n",
    "                criteria_divs = container.find_elements(By.XPATH, \".//div[@title]\")\n",
    "                for div in criteria_divs:\n",
    "                    title = div.get_attribute('title')\n",
    "                    if title and title.strip():\n",
    "                        criteria_set.add(title.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Approach 1 failed: {e}\")\n",
    "        \n",
    "        # Approach 2: Direct search for divs with specific classes and title attributes\n",
    "        try:\n",
    "            criteria_divs = driver.find_elements(\n",
    "                By.XPATH, \n",
    "                \"//div[contains(@class, 'border-green') and @title]\"\n",
    "            )\n",
    "            \n",
    "            for div in criteria_divs:\n",
    "                title = div.get_attribute('title')\n",
    "                if title and title.strip():\n",
    "                    criteria_set.add(title.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Approach 2 failed: {e}\")\n",
    "            \n",
    "        # If criteria were found, join them into a comma-separated string\n",
    "        if criteria_set:\n",
    "            return \", \".join(sorted(criteria_set))  # Sort for consistent output\n",
    "        else:\n",
    "            return \"No eligibility criteria found\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting eligibility criteria from {url}: {e}\")\n",
    "        return \"Error extracting eligibility criteria\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process scheme links and extract the Scheme Type and Eligibility Criteria.\n",
    "    \"\"\"\n",
    "    input_df = load_input()\n",
    "    if input_df is None:\n",
    "        return\n",
    "    print(f\"Loaded {len(input_df)} schemes from {INPUT_FILE}.\")\n",
    "    \n",
    "    checkpoint_df, processed_links = load_checkpoint()\n",
    "    print(f\"Already processed {len(processed_links)} scheme links from checkpoint.\")\n",
    "    \n",
    "    # Create new columns if they don't exist\n",
    "    if \"scheme_type\" not in input_df.columns:\n",
    "        input_df[\"scheme_type\"] = \"\"\n",
    "    if \"eligibility_criteria\" not in input_df.columns:\n",
    "        input_df[\"eligibility_criteria\"] = \"\"\n",
    "    \n",
    "    # Process only schemes that haven't been processed\n",
    "    unprocessed_df = input_df[~input_df[\"scheme_link\"].isin(processed_links)]\n",
    "    print(f\"Processing {len(unprocessed_df)} new schemes for extraction.\")\n",
    "    \n",
    "    updated_rows = []\n",
    "    total_to_process = len(unprocessed_df)\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    for idx, row in tqdm(unprocessed_df.iterrows(), total=total_to_process, desc=\"Processing Schemes\"):\n",
    "        scheme_name = row[\"scheme_name\"]\n",
    "        scheme_link = row[\"scheme_link\"]\n",
    "        \n",
    "        if not scheme_link or not scheme_link.startswith(\"http\"):\n",
    "            print(f\"Skipping invalid URL for: {scheme_name}\")\n",
    "            updated_row = row.copy()\n",
    "            updated_row[\"scheme_type\"] = \"Error: Invalid URL\"\n",
    "            updated_row[\"eligibility_criteria\"] = \"Error: Invalid URL\"\n",
    "            updated_rows.append(updated_row)\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing scheme: {scheme_name}\")\n",
    "        print(f\"URL: {scheme_link}\")\n",
    "        \n",
    "        # Extract scheme type with retries\n",
    "        scheme_type = None\n",
    "        eligibility_criteria = None\n",
    "        retries = 0\n",
    "        backoff_time = 2\n",
    "        \n",
    "        while (scheme_type is None or eligibility_criteria is None) and retries < MAX_RETRIES:\n",
    "            try:\n",
    "                if scheme_type is None:\n",
    "                    scheme_type = extract_scheme_type(driver, scheme_link)\n",
    "                \n",
    "                if eligibility_criteria is None:\n",
    "                    # Add a small delay before extracting eligibility criteria\n",
    "                    time.sleep(1)\n",
    "                    eligibility_criteria = extract_eligibility_criteria(driver, scheme_link)\n",
    "                \n",
    "                # Check if both extractions were successful\n",
    "                if (not scheme_type or len(scheme_type) < 3 or \"Not Available\" in scheme_type or\n",
    "                    not eligibility_criteria or \"No eligibility\" in eligibility_criteria or \"Error\" in eligibility_criteria):\n",
    "                    print(f\"Attempt {retries+1}: Insufficient info for {scheme_name}\")\n",
    "                    retries += 1\n",
    "                    if retries < MAX_RETRIES:\n",
    "                        print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                        time.sleep(backoff_time)\n",
    "                        backoff_time *= 1.5\n",
    "                        if \"Not Available\" in scheme_type or len(scheme_type) < 3:\n",
    "                            scheme_type = None\n",
    "                        if \"No eligibility\" in eligibility_criteria or \"Error\" in eligibility_criteria:\n",
    "                            eligibility_criteria = None\n",
    "                    else:\n",
    "                        if scheme_type is None:\n",
    "                            scheme_type = \"Section Not Available against this scheme\"\n",
    "                        if eligibility_criteria is None:\n",
    "                            eligibility_criteria = \"No eligibility criteria found\"\n",
    "                else:\n",
    "                    print(f\"Successfully extracted scheme type: {scheme_type}\")\n",
    "                    print(f\"Successfully extracted eligibility criteria: {eligibility_criteria}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {retries+1} failed for {scheme_name}: {e}\")\n",
    "                retries += 1\n",
    "                if retries < MAX_RETRIES:\n",
    "                    print(f\"Waiting {backoff_time} seconds before retry...\")\n",
    "                    time.sleep(backoff_time)\n",
    "                    backoff_time *= 1.5\n",
    "                else:\n",
    "                    if scheme_type is None:\n",
    "                        scheme_type = \"Section Not Available against this scheme\"\n",
    "                    if eligibility_criteria is None:\n",
    "                        eligibility_criteria = \"No eligibility criteria found\"\n",
    "        \n",
    "        updated_row = row.copy()\n",
    "        updated_row[\"scheme_type\"] = scheme_type\n",
    "        updated_row[\"eligibility_criteria\"] = eligibility_criteria\n",
    "        updated_rows.append(updated_row)\n",
    "        \n",
    "        if (idx + 1) % 5 == 0 or (idx + 1) == total_to_process:\n",
    "            processed_df = pd.DataFrame(updated_rows)\n",
    "            temp_df = pd.concat([checkpoint_df, processed_df], ignore_index=True)\n",
    "            temp_df.to_excel(CHECKPOINT_FILE, index=False)\n",
    "            print(f\"Checkpoint saved. Processed {idx+1}/{total_to_process} schemes.\")\n",
    "        \n",
    "        delay_time = 2 + random.random() * 3\n",
    "        print(f\"Waiting {delay_time:.2f} seconds before next scheme...\")\n",
    "        time.sleep(delay_time)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    final_df = pd.concat([checkpoint_df, pd.DataFrame(updated_rows)], ignore_index=True)\n",
    "    if \"original_index\" in final_df.columns:\n",
    "        final_df = final_df.sort_values(\"original_index\")\n",
    "    final_df.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nExtraction complete! Updated {len(updated_rows)} new schemes.\")\n",
    "    print(f\"Results saved to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
